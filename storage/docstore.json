{"docstore/metadata": {"c95ed6b3-156f-4e59-8a0d-01c8133828ce": {"doc_hash": "087d7cdc1493d6565a6ffe0fc035f005ebe8d7601b5387197d3c7a18fefae9ec"}, "06548b42-8cef-403a-ad7d-f5bc17bac1ea": {"doc_hash": "bac9924f539ef142e2f43f38ffe4a0637c2c6dcc1df22022455d95c895670a29"}, "477b3e7f-34d7-42c6-94dd-20939b287d23": {"doc_hash": "fe41584a62de15948b1ced183b948c13152333e29fbeb55e23e52e02351c05cc"}, "0aa1876e-2be2-438e-884f-77f133c1b9be": {"doc_hash": "ff61c384932e85b2cf6c2c32084a429ef9f66a8a4b6aa869bdf6396ec21c5753"}, "fe3f9758-e169-41e7-a420-d2f454a7b3b1": {"doc_hash": "9f9476953591ac7752b186fa188864d1990a2603eec941706d407ee66e9e4017"}, "e2152268-5410-4733-b6a1-cfa6c355e475": {"doc_hash": "eaa198ec0e25d2b44fbb1a0d20ea2ee79fae1607fa728b74e534c3366f444359"}, "17656815-0671-4f39-8e55-84ef48e7fce8": {"doc_hash": "448e601f0c7b09b2b543b0fba5aea7f6de6a81483c43be161bb5c4370dacf867"}, "32288d9e-325e-4485-9220-ed86c5897834": {"doc_hash": "77a8aa540044fc49437e9b534e7a2492f4da2a65d66af226dc6fe0888ec316a6"}, "4a4e89b4-63f8-46a9-8041-e539607181b3": {"doc_hash": "27e16056238cc63602e56a8f835cbc699cf7b727bf880af91f62d6bdc264c581"}, "966bf940-e06a-4294-96b8-1c6f319525b0": {"doc_hash": "036fc22187e04084a7f40791d85fbfb6a8bd88a38dfee19d1e5bdec83432f0c3"}, "5343348a-9323-40cf-add1-f852bfe54ddc": {"doc_hash": "402f8b39e2cbb3d75db6e715206dadcbeb32db4b53a7928a9a4f1858f2be8768"}, "73b1df26-3a2f-40bc-876a-a267b3b5ded5": {"doc_hash": "c22bff5b3086d9450070731b6b9af90112b2410916ac3050acc83599c552dbfd"}, "fc365208-fe12-4f62-8f10-ed1b66d04c1b": {"doc_hash": "4df274c5f1457e1338b358606ff772f67599dac2ab1a9b0e6748f4860d8b19e3"}, "6f5d3c79-be03-473b-98bb-d11af3a1e2ef": {"doc_hash": "92c0c038d44a8235201201de868eefbb99bf40a5717a3378a1ff0881e8d80569"}, "61670ee1-a3d2-4abb-967a-11acf5e7462e": {"doc_hash": "be0b299a7dba161307b25db69e9688b2c2baeaf4f15c22a3ede62b0a8f5e0449"}, "e7b68b7d-f8b6-4cbb-884b-ac86a27a6728": {"doc_hash": "a9a985a30e1614acddcf7c81561396f45775ed4304cbde3a94ef5ac702c7be45"}, "2d51d47a-d405-43bc-a8ca-9de1732f8548": {"doc_hash": "98eb92e2c2ae2a589f8ba11cc1eb3545a577c2c61b0a8cfdc49c9c7601f22d54"}, "8ecc7d75-5871-4171-a74c-64872af2d05e": {"doc_hash": "aed0f667c42251c3f99f49720d0b4f761b9e93c75d30fcdaa00aac4e08112ea3"}, "517c3417-3654-4b8a-bcbd-e5f226c1cff1": {"doc_hash": "0414edfb8bcfcaafc2a5206b8488cc0f07ec694fa622f57d14532ff03f303a77"}, "d962463e-097f-4370-83d8-e20b4f1092b5": {"doc_hash": "087d7cdc1493d6565a6ffe0fc035f005ebe8d7601b5387197d3c7a18fefae9ec", "ref_doc_id": "c95ed6b3-156f-4e59-8a0d-01c8133828ce"}, "47b0b74f-b680-46ea-bd0f-915a2be19cec": {"doc_hash": "fb8f85b68c59e17164cde5787f453bff7441e0c003260f38d67eb6fac417d1b2", "ref_doc_id": "06548b42-8cef-403a-ad7d-f5bc17bac1ea"}, "96e962f9-5938-4f73-9b8b-2455ca8aed43": {"doc_hash": "a46291e8a7e6aedf38d6cec4109fa7331ecb1ea385a7f769cced199443ec3952", "ref_doc_id": "06548b42-8cef-403a-ad7d-f5bc17bac1ea"}, "97aeea4d-d204-40c4-8acf-3ee0db955769": {"doc_hash": "1f0995d014bd1d21318190d144e993dba8310a9ff9dec6ef0725f064f7bcd5a6", "ref_doc_id": "477b3e7f-34d7-42c6-94dd-20939b287d23"}, "f970a434-27b3-4847-a6e9-cc2a55745a11": {"doc_hash": "c9f03ae2628f6a9d3612768384ae8b11fe2f29d0c3a9fc145865472293d4f002", "ref_doc_id": "477b3e7f-34d7-42c6-94dd-20939b287d23"}, "e9b7a385-56a6-47a9-b30a-60712e34e4b1": {"doc_hash": "1b2e28eeeaed9b5057fa2181a521700d4d3e84e18ca0958d2dd63173425c615f", "ref_doc_id": "0aa1876e-2be2-438e-884f-77f133c1b9be"}, "bfa1faf9-e0d7-4773-ad67-24844e8b97e3": {"doc_hash": "3fd69c456f81628ec1140f654ad34f34d42ba56f21de483042f9b174b8219d24", "ref_doc_id": "0aa1876e-2be2-438e-884f-77f133c1b9be"}, "006d47bc-6903-4d82-92ca-f5699b7f2a8b": {"doc_hash": "2986b86c1c22d139fa34e060e42ca89b4e9cc682d5e91b28efb5dc102a878934", "ref_doc_id": "fe3f9758-e169-41e7-a420-d2f454a7b3b1"}, "726b4a7f-3aad-4b77-ba49-4663303a08fb": {"doc_hash": "3781967c692772b2d753ebe64347a032e56de98d8eab554e6485c1fde4b19df7", "ref_doc_id": "fe3f9758-e169-41e7-a420-d2f454a7b3b1"}, "51431c2b-5e97-4d60-8ca2-d67984a06cb5": {"doc_hash": "8cd5e6049ffd0dbe0f49434f9696a16e75266943f6165689b07b37a3585432ca", "ref_doc_id": "e2152268-5410-4733-b6a1-cfa6c355e475"}, "e98237d3-8b3d-402b-965e-de059acd5ff5": {"doc_hash": "2b3b2db4a72c2dfa61a9b194f2fa7c35017e476de7b758392236896900494244", "ref_doc_id": "e2152268-5410-4733-b6a1-cfa6c355e475"}, "a8bc167f-b8e3-4e00-9a95-de487a6e88aa": {"doc_hash": "453a4fc164c6a626088fa198c95fc534ed3f390c6a573023658f1f4462c9885d", "ref_doc_id": "17656815-0671-4f39-8e55-84ef48e7fce8"}, "b170b30b-8df7-4d10-bd7d-2a0930d088d7": {"doc_hash": "f6bc875ecd6f2657c68a5aeafc50a7426d18a358bce2fb8288c367fa75b262af", "ref_doc_id": "17656815-0671-4f39-8e55-84ef48e7fce8"}, "13db9588-5abd-4d5a-8f95-69846d49d770": {"doc_hash": "e1fb04a44da86176f1f1e339a8adc7edd8f5218009d33c1b1a519cb8b6198ad5", "ref_doc_id": "32288d9e-325e-4485-9220-ed86c5897834"}, "d5dd142a-4ecc-43ea-bb30-960fb0e15d2d": {"doc_hash": "20a6faadfacf3e1803bc73625db43470186f1f574974213f2af53d5606769b4a", "ref_doc_id": "32288d9e-325e-4485-9220-ed86c5897834"}, "bee1e4ce-2c73-4d96-ba76-be60444c7aa9": {"doc_hash": "4d5ca1f718055bc141d7c53a480083c1cc5652e3df52bc217ec76609fd6c70dc", "ref_doc_id": "4a4e89b4-63f8-46a9-8041-e539607181b3"}, "64342766-ba3c-4302-9c57-b8ac3418af80": {"doc_hash": "e4e4c7c6b0856797709de5c7b589d392911c016b9092e092b89912af3bad9dfb", "ref_doc_id": "4a4e89b4-63f8-46a9-8041-e539607181b3"}, "b6909f54-10d8-47bc-b176-324f77502f86": {"doc_hash": "207bd42f19522ea418c85a9ff0c53f52a321f20f24b4a1e5e734c35457974c4e", "ref_doc_id": "966bf940-e06a-4294-96b8-1c6f319525b0"}, "7ad45a9b-6e1c-4f39-96a5-3bc4b6e5e047": {"doc_hash": "ac8deff0fe02d7a6d2cabc2d1f23e8cf37556f10a0ba844f3c68cc70f3c409f2", "ref_doc_id": "966bf940-e06a-4294-96b8-1c6f319525b0"}, "3873ba07-bdea-4dd9-b60f-91e17a28e327": {"doc_hash": "3348556f1bca51fdbb3687a15e4816d5aed4e68546cff3d0febee815def43e12", "ref_doc_id": "5343348a-9323-40cf-add1-f852bfe54ddc"}, "1ebd5849-4bfb-4479-899b-9be3b03c7f9f": {"doc_hash": "2dc86bd710dd4af917246a18cef6613330fdd609612486f7bd44fb6260516b80", "ref_doc_id": "5343348a-9323-40cf-add1-f852bfe54ddc"}, "b9e919ab-5906-4fda-8a12-06e1feb115ff": {"doc_hash": "3661d5f768029fb689743af0ea83cf2334cbf2a39fdfe8f9394ec84b248e12d3", "ref_doc_id": "73b1df26-3a2f-40bc-876a-a267b3b5ded5"}, "a1bfaad4-78d3-4411-bc23-8a947fa7bceb": {"doc_hash": "72553fb40ef752784169a8d50cb3116b6953ba23bac474d1428a612901193aec", "ref_doc_id": "73b1df26-3a2f-40bc-876a-a267b3b5ded5"}, "84edadf0-4632-4891-b206-b2bc13b350d6": {"doc_hash": "eb5f604423f930828436a5d253fccfb31c10b92b2a76dce865c7724b815ecad5", "ref_doc_id": "fc365208-fe12-4f62-8f10-ed1b66d04c1b"}, "6261588e-302b-4a95-aa43-0ded02369dea": {"doc_hash": "51dd347d226eb926e869b12381e211675c73e3b721e82e3d214d0fe8ebf7740a", "ref_doc_id": "fc365208-fe12-4f62-8f10-ed1b66d04c1b"}, "8b1417e5-1925-457b-b055-10dd94cb9927": {"doc_hash": "fe1691389aecfb67d05ec0b43c41cd5e824935fb1297f825197731460f19aaea", "ref_doc_id": "6f5d3c79-be03-473b-98bb-d11af3a1e2ef"}, "b7744b70-a29c-4997-a953-4cdbd7f0cee1": {"doc_hash": "3fcc0a80444ae59292c3adba336dc18648cfaf1766d88ffb548dfac0f5479605", "ref_doc_id": "6f5d3c79-be03-473b-98bb-d11af3a1e2ef"}, "8eb97403-a869-4bdf-a5d8-31f7260f557a": {"doc_hash": "10952c66aa57dcf651bd852fe19c182ed7ed467b16b0be83c599c4dafece23d5", "ref_doc_id": "61670ee1-a3d2-4abb-967a-11acf5e7462e"}, "d859464f-8e96-4d9d-8e25-d5d76e4c4ec9": {"doc_hash": "d807f55e793ed0159ff9ef9e65e6c18c60e9ce08e3d0420abf8ee5ce5162758a", "ref_doc_id": "61670ee1-a3d2-4abb-967a-11acf5e7462e"}, "b0531301-e088-4b8b-a2da-22c0b1397343": {"doc_hash": "4d645c987e1a41955fc88ef8718c045171ab2a9ad50d6e7b8e303141388efaa4", "ref_doc_id": "e7b68b7d-f8b6-4cbb-884b-ac86a27a6728"}, "ac6c9336-7018-445c-9555-cae5cf448e22": {"doc_hash": "03d2102f2640892b8d9c663d4bf942c9aea9b830092c3650bcea7dc7a6011383", "ref_doc_id": "e7b68b7d-f8b6-4cbb-884b-ac86a27a6728"}, "bd4c514c-6885-4f11-9378-ef1c4d37927f": {"doc_hash": "acc3f3e053d3238e943c3b22a42552fc75da70b31e9d1417abfba8ada6c63e31", "ref_doc_id": "2d51d47a-d405-43bc-a8ca-9de1732f8548"}, "ca794b48-97b4-438b-b9cc-2af186639bcc": {"doc_hash": "ad94f8d6650d51094608d5386642634acb852158cc4335e7c68cc096c59944e5", "ref_doc_id": "2d51d47a-d405-43bc-a8ca-9de1732f8548"}, "3264478c-66c0-4c4a-b218-35512f5a2179": {"doc_hash": "a5bca08c4f41400b4d98c26d6b9cbcfb0c02066195be75921577983c67a10eac", "ref_doc_id": "8ecc7d75-5871-4171-a74c-64872af2d05e"}, "85948b7c-d794-41f6-a9d7-bf083d155548": {"doc_hash": "902eaaeda7a52e486e379d635c6a1f6863df6c9176f3b81e7422a8ab64c8ce75", "ref_doc_id": "8ecc7d75-5871-4171-a74c-64872af2d05e"}, "3327329d-4bdc-4787-901d-047cb1eb6bbe": {"doc_hash": "3e25c0b2fabd045f2555e671d2ba06f630f81dcdc96cbfe82795bab763eec7f2", "ref_doc_id": "517c3417-3654-4b8a-bcbd-e5f226c1cff1"}, "723cfa19-9362-42a9-b04f-f1688916185d": {"doc_hash": "9d9269722404db85cfa5b64d83213e28e5a1b8a59051a3442ccf0f4642ab7522", "ref_doc_id": "517c3417-3654-4b8a-bcbd-e5f226c1cff1"}}, "docstore/data": {"d962463e-097f-4370-83d8-e20b4f1092b5": {"__data__": {"id_": "d962463e-097f-4370-83d8-e20b4f1092b5", "embedding": null, "metadata": {"page_label": "1", "file_name": "paper.pdf", "file_path": "eigene/paper.pdf", "creation_date": "2023-11-09", "last_modified_date": "2023-10-20", "last_accessed_date": "2023-11-10"}, "excluded_embed_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "c95ed6b3-156f-4e59-8a0d-01c8133828ce", "node_type": "4", "metadata": {"page_label": "1", "file_name": "paper.pdf", "file_path": "eigene/paper.pdf", "creation_date": "2023-11-09", "last_modified_date": "2023-10-20", "last_accessed_date": "2023-11-10"}, "hash": "087d7cdc1493d6565a6ffe0fc035f005ebe8d7601b5387197d3c7a18fefae9ec"}}, "hash": "087d7cdc1493d6565a6ffe0fc035f005ebe8d7601b5387197d3c7a18fefae9ec", "text": "Please quote as: Schmitt, Anuschka; Zierau, Naim; Janson, Andreas & Leimeister, \nJan Marco: Voice as a Contemporary Frontier of Interaction Design. 2021. - \nEuropean Conference on Information Systems (ECIS). - Virtual.\nElectronic copy available at: https://ssrn.com/abstract=3910609", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "47b0b74f-b680-46ea-bd0f-915a2be19cec": {"__data__": {"id_": "47b0b74f-b680-46ea-bd0f-915a2be19cec", "embedding": null, "metadata": {"page_label": "2", "file_name": "paper.pdf", "file_path": "eigene/paper.pdf", "creation_date": "2023-11-09", "last_modified_date": "2023-10-20", "last_accessed_date": "2023-11-10"}, "excluded_embed_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "06548b42-8cef-403a-ad7d-f5bc17bac1ea", "node_type": "4", "metadata": {"page_label": "2", "file_name": "paper.pdf", "file_path": "eigene/paper.pdf", "creation_date": "2023-11-09", "last_modified_date": "2023-10-20", "last_accessed_date": "2023-11-10"}, "hash": "bac9924f539ef142e2f43f38ffe4a0637c2c6dcc1df22022455d95c895670a29"}, "3": {"node_id": "96e962f9-5938-4f73-9b8b-2455ca8aed43", "node_type": "1", "metadata": {"page_label": "2", "file_name": "paper.pdf", "file_path": "eigene/paper.pdf", "creation_date": "2023-11-09", "last_modified_date": "2023-10-20", "last_accessed_date": "2023-11-10"}, "hash": "a46291e8a7e6aedf38d6cec4109fa7331ecb1ea385a7f769cced199443ec3952"}}, "hash": "fb8f85b68c59e17164cde5787f453bff7441e0c003260f38d67eb6fac417d1b2", "text": "Twenty -Ninth European Conference on Information Systems (ECIS 2021), A Virtual AIS Conference.      1 VOICE  AS A CONTEM PORARY  FRONTIER  \nOF INTERACTION DESIGN  \nResearch Paper \nAnuschka Schmitt , University of St. Gallen, St.Gallen, Switzerland,  \nanusch ka.schmit t@unisg.ch  \nNaim Zierau , University of St.Gallen, St.  Gallen, Switzerland,  naim.zierau@unisg.ch  \nAndrea s Janson , University of St. Gallen, St.Gallen, Switzerland,  andreas.janson @unisg.ch  \nJan Marco Leimeister , University of St. Gallen, St.Gallen, Switzerland,  \njanmarco.leimeister @unisg.ch  \nAbstract  \nVoice  assistants \u2019 increasingly nuanced and natural communicatio n bears new opportunities for user \nexperiences  and task automation , while  challenging existing  patterns  of hu man-computer interact ion. \nA fra gmented  research field , as well as constant technological advancements , impede a common \napprehension  of prevalent de sign features  of voice -based interfaces . As part of this  study, 86 papers  \nacross domains  are systematically ide ntified and analysed  to arrive at a common understanding o f \nvoice assi stants.  The review highlights perceptual differences to other human -compute r interfaces  and \npoints out relevant auditory  cues. Key fin dings regarding  those cues \u2019 impact on user perception  and \nbehaviour  are discu ssed along with the  three design strategies  1) person ification, 2) individualization  \nand 3) contextualization . Avenues f or future research are lastly dedu cted. Our results provid e relev ant \nopportun ities to research ers and  designers alike to advan ce the d esign and deployment of voice \nassistants.   \n \nKeywords: Voice Assistant, Conversational Interface , Voice Design, Human -Computer Intera ction  \n1 Introduction  \nIn 2013, the ever-present  and empat hic artificial  voice p ersonality , which accompan ied the protag onist \nTheodore  Twombly  in the Sci -Fi Dram a \u201cHer\u201d, seemed  shockingly  imaginable, yet  still far from \nreality. With the in tegration of voice assistan ts (VAs) in mobile devic es, personal homes , and service \ninterfa ces, voice  has arrived  in our eve ryday life , just as it has i n Twombly \u2019s. Amazo n\u2019s Alexa , for \ninstance , learns from mistak es, is takin g breathi ng breaks , adapt s her tone of voi ce to  the c ontex t of the \nconversation  and proact ively offer s her help  (Low, 202 0; Tarantola , 2020) . In sho rt, interact ions with \nVAs promise  to be incre asingly  natur al and effortless.  Adoption and use numbers  underline the wide -\nspread accep tance of such ag ents: With over 110 mil lion users in the US alone, the number of VAs is \nexpecte d to reach 8.4 billion by 2024 (Smith, 2020) . More interesting ly, one is quick to imagi ne the \npotential of VAs: The hands- and eyes -free nature of VAs offers promising  applications  for certain \nuser groups  and contexts , especially against the back drop of 2.2 million people with vision impairment \nworldwide  (Abdolrahmani et al., 2018; World Health Organizati on, 2019) . \nWhile the fields of ap plication see m endle ss and the implementati on benef its intuitive , the empirical \nunderstanding of VAs is not as straightforwar d. Research  on VAs is growing rapidly yet is highly \nfragmented  in terms of publication outlet . As a result, two key research chall enges arise : First,  \ndifferent concep tualizations  of voice -based agents e xist.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "96e962f9-5938-4f73-9b8b-2455ca8aed43": {"__data__": {"id_": "96e962f9-5938-4f73-9b8b-2455ca8aed43", "embedding": null, "metadata": {"page_label": "2", "file_name": "paper.pdf", "file_path": "eigene/paper.pdf", "creation_date": "2023-11-09", "last_modified_date": "2023-10-20", "last_accessed_date": "2023-11-10"}, "excluded_embed_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "06548b42-8cef-403a-ad7d-f5bc17bac1ea", "node_type": "4", "metadata": {"page_label": "2", "file_name": "paper.pdf", "file_path": "eigene/paper.pdf", "creation_date": "2023-11-09", "last_modified_date": "2023-10-20", "last_accessed_date": "2023-11-10"}, "hash": "bac9924f539ef142e2f43f38ffe4a0637c2c6dcc1df22022455d95c895670a29"}, "2": {"node_id": "47b0b74f-b680-46ea-bd0f-915a2be19cec", "node_type": "1", "metadata": {"page_label": "2", "file_name": "paper.pdf", "file_path": "eigene/paper.pdf", "creation_date": "2023-11-09", "last_modified_date": "2023-10-20", "last_accessed_date": "2023-11-10"}, "hash": "fb8f85b68c59e17164cde5787f453bff7441e0c003260f38d67eb6fac417d1b2"}}, "hash": "a46291e8a7e6aedf38d6cec4109fa7331ecb1ea385a7f769cced199443ec3952", "text": "Second, it becomes challen ging to interpr et \nand predict user perceptions  and behaviour for different deployment conte xts. The f ragmented \nresearch body calls for a cont ext-sensitive knowledge base on v oice design . Existing fram eworks and \nElectronic copy available at: https://ssrn.com/abstract=3910609", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "97aeea4d-d204-40c4-8acf-3ee0db955769": {"__data__": {"id_": "97aeea4d-d204-40c4-8acf-3ee0db955769", "embedding": null, "metadata": {"page_label": "3", "file_name": "paper.pdf", "file_path": "eigene/paper.pdf", "creation_date": "2023-11-09", "last_modified_date": "2023-10-20", "last_accessed_date": "2023-11-10"}, "excluded_embed_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "477b3e7f-34d7-42c6-94dd-20939b287d23", "node_type": "4", "metadata": {"page_label": "3", "file_name": "paper.pdf", "file_path": "eigene/paper.pdf", "creation_date": "2023-11-09", "last_modified_date": "2023-10-20", "last_accessed_date": "2023-11-10"}, "hash": "fe41584a62de15948b1ced183b948c13152333e29fbeb55e23e52e02351c05cc"}, "3": {"node_id": "f970a434-27b3-4847-a6e9-cc2a55745a11", "node_type": "1", "metadata": {"page_label": "3", "file_name": "paper.pdf", "file_path": "eigene/paper.pdf", "creation_date": "2023-11-09", "last_modified_date": "2023-10-20", "last_accessed_date": "2023-11-10"}, "hash": "c9f03ae2628f6a9d3612768384ae8b11fe2f29d0c3a9fc145865472293d4f002"}}, "hash": "1f0995d014bd1d21318190d144e993dba8310a9ff9dec6ef0725f064f7bcd5a6", "text": "Schmitt et al. / Voice Contemporary Frontier Int eraction De sign \nTwenty -Ninth European Conference on Information Systems (ECIS 2021), A Virtual AIS Conference.      2 categori sations of design -relevant  features f or VAs are characteriz ed by an either  isolated perspe ctive  \nfocusing on certain types  of VAs only, or by considering VAs as one ty pe of convers ationa l agents and \nsubseque ntly black -boxing voice -specific  cues (Cambre & Kulkarni, 2019) . Potential for VAs has \nbeen  recognized  with resear chers from different domains alik e callin g to explore how to design  a \nconvers ational interfac e, and how to consecutively evaluate  user satisfaction  (McTear, 201 7). Pfeuffer, \nBenlian, Gimpel and Hinz  (2019)  raise the import ance o f considering individual auditor y featu res of \nsuch assistant s. This study aims  to contribu te to the field of IS by bringing to gether disparate resear ch \nstrea ms on voice-based conversational agents  towards a more holi stic perspective  on related design \ncues. More precisely, auditory cues found in VAs are mapped against related user p erception  and \ninteraction outcomes  for a more integrated and context -sensitive  analysis of VA design . Ultima tely, \nthis study poses t he following research  question:  \nWha t are voice -relevant cues of VAs and how can the y be leveraged for intera ction  design? \nThis paper is organ ized as follows. Section 2 presents the conceptual backgroun d, followed by section  \n3 illustrat ing the method of the systematic literature review. Section 4 reviews the identif ied literature  \nand discusses main f indings along three  design strategies. A discuss ion a nd research agenda  are \npresented in sect ion 5.  Finally, conclusions and limitat ions are poin ted out.  \n2 Conceptual Backgrou nd \n2.1 Voice as an Interaction Modality  \nVAs, which are also referred to as  convers ational agents , or dialogue systems  (Balasuriya et al., 2018; \nO\u2019Brien et al., 2020) , are computer programs t hat inte ract w ith cu stomers via  autom atic sp eech \nrecognition  and natural l angu age processing in  form of voi ce-mediat ed communicatio n (Pfeuffer et al., \n2019) . The increasingly sophisti cated interaction quality of VAs and omnipresence of such interfaces \nhas been propelled by ongoing advances in Artificia l Intelligence (AI) . However, w ith a plethora of \nexisting personal agents and conversa tional interfaces, what m akes voice so different?  \nAs il lustrated in Table 1, the nature of voice exhibits  several features  which  ultimately  differentiate \nvoice -based interaction from other prevalent interface modalities , such as text. First, voice -based  \ninteraction  follows a sequential process ing, which resembles  natural human communication. Text -\nbased conversation , on the other hand,  allows  to re-read and re -write messages , which, however, also \nintroduces less natural  pauses  (Rubin et al., 2000) . Second, voice -based interac tion is marked by mor e \ncolloquial l anguage, which allows for intu itive expression and  reception of information  (Dennis et al., \n2008) . Third , voice encompa sses signals beyo nd mere information provi sion and th us changes the \nnatur e of interaction  betwe en users  and a conversational agent  sustainabl y (Rosenth al & Ryan, 2000) . \nVoice not only exh ibits a greater variety and intuitively  appropriated cues, yet also , and most \nimportantly, a great extent of non verbal cues  (Redeker, 1984) . In fact, voice has been  defined as  \u201cthe \ncarrier of speech \u201d (Belin,  Fecteau and B\u00e9dard, 2004 , p.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "f970a434-27b3-4847-a6e9-cc2a55745a11": {"__data__": {"id_": "f970a434-27b3-4847-a6e9-cc2a55745a11", "embedding": null, "metadata": {"page_label": "3", "file_name": "paper.pdf", "file_path": "eigene/paper.pdf", "creation_date": "2023-11-09", "last_modified_date": "2023-10-20", "last_accessed_date": "2023-11-10"}, "excluded_embed_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "477b3e7f-34d7-42c6-94dd-20939b287d23", "node_type": "4", "metadata": {"page_label": "3", "file_name": "paper.pdf", "file_path": "eigene/paper.pdf", "creation_date": "2023-11-09", "last_modified_date": "2023-10-20", "last_accessed_date": "2023-11-10"}, "hash": "fe41584a62de15948b1ced183b948c13152333e29fbeb55e23e52e02351c05cc"}, "2": {"node_id": "97aeea4d-d204-40c4-8acf-3ee0db955769", "node_type": "1", "metadata": {"page_label": "3", "file_name": "paper.pdf", "file_path": "eigene/paper.pdf", "creation_date": "2023-11-09", "last_modified_date": "2023-10-20", "last_accessed_date": "2023-11-10"}, "hash": "1f0995d014bd1d21318190d144e993dba8310a9ff9dec6ef0725f064f7bcd5a6"}}, "hash": "c9f03ae2628f6a9d3612768384ae8b11fe2f29d0c3a9fc145865472293d4f002", "text": "129 ) as it becomes distinguishab le from  other \nmodalities  by signalling  social cues beyon d the  pure verbal i nformation communicated  (i.e. content) . \nIn that sense, voice provide s cues about the  personality and identity (i.e., gender, a ge), as well as \nemotions (i.e., happiness, anger) , of a speaker (Sutton et al., 2019) . Overall, distinguishing  features of \nvoice -based interaction  offer novel opportunities for VAs as  an interactio n interface.  \n \n Text -Based Interac tions Voice -Based Interactions  \nInteraction Flow  Permanent, parallel processing  Temporal, sequential processing  \nLanguage   \n& Sy ntax More complex vocabulary   \nand sentence  structu re Simpler , colloquial vocabular y  \nImprecise sentence structur e \nNatural ness Limited  extent o f nonve rbal cues  Great extent of nonv erbal cues  \nTable 1. Key differentiating features  of text- and voice -based interaction . \nElectronic copy available at: https://ssrn.com/abstract=3910609", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "e9b7a385-56a6-47a9-b30a-60712e34e4b1": {"__data__": {"id_": "e9b7a385-56a6-47a9-b30a-60712e34e4b1", "embedding": null, "metadata": {"page_label": "4", "file_name": "paper.pdf", "file_path": "eigene/paper.pdf", "creation_date": "2023-11-09", "last_modified_date": "2023-10-20", "last_accessed_date": "2023-11-10"}, "excluded_embed_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0aa1876e-2be2-438e-884f-77f133c1b9be", "node_type": "4", "metadata": {"page_label": "4", "file_name": "paper.pdf", "file_path": "eigene/paper.pdf", "creation_date": "2023-11-09", "last_modified_date": "2023-10-20", "last_accessed_date": "2023-11-10"}, "hash": "ff61c384932e85b2cf6c2c32084a429ef9f66a8a4b6aa869bdf6396ec21c5753"}, "3": {"node_id": "bfa1faf9-e0d7-4773-ad67-24844e8b97e3", "node_type": "1", "metadata": {"page_label": "4", "file_name": "paper.pdf", "file_path": "eigene/paper.pdf", "creation_date": "2023-11-09", "last_modified_date": "2023-10-20", "last_accessed_date": "2023-11-10"}, "hash": "3fd69c456f81628ec1140f654ad34f34d42ba56f21de483042f9b174b8219d24"}}, "hash": "1b2e28eeeaed9b5057fa2181a521700d4d3e84e18ca0958d2dd63173425c615f", "text": "Schmitt et al. / Voice Contemporary Frontier Int eraction De sign \nTwenty -Ninth European Conference on Information Systems (ECIS 2021), A Virtual AIS Conference.      3 Against the backdrop  of technological ad vancements , established theori es and previously made  \ncomparisons between text and voice -based interaction  might only  hold to a certain extent . In that \nsense, both text - and voic e-based interfaces enable a more dynamic , one-to-one intera ction between \nusers and machines  as compa red to non-conversational digital  interfaces  based on  1) a dialog ue flow \nthat mimics human -to-human communic ation through sequentia l turn-taking  2) and enrich ed langu age \nwith nonverbal , social cues (Hildebrand & Bergner, 2020) . State-of-the-art text -based c onversational \ninterfa ces, such as chatbo ts, also offer nonverbal cues , including reading and typing delays , typ ing \nindicato rs or emojis , which ultimately contr ibute to the natura lness of an interaction  (Schuetzler et al., \n2021) . However, audito ry cues enhance characteristics of natu ral interac tion, as wel l as arouse mental \nsimulation  of contextu al as pects  other mo dalities are inca pable of (Griffin et al., 2018) . In the \nfollowi ng, these  differentiating properties of nonverbal  auditory cues are further delineated . \n2.2 Voice Design   \nVoice design is studied  in human -robot in teraction, psychology, phonetics an d linguistic s and has \ntaken a closer  look at auditory cues, such as  speed rate  or pitch , to under stand  how such cues and their \nrespective m odification  affect u ser perception  and behavio ur (Chidambaram et al., 2012; Elkins & \nDerrick, 2013) . In this pape r, we are conc erned with such non-linguistic acoustics (how vo cal sounds \nand speech is heard),  as opposed to verba l content and s tyle. We explicitly exclude t he latter , which  \nrefers to th e literal meaning o f a message and how this message is expressed  in terms of lexical \ndiversity (Feine et al., 2019) .  \nSutton  et al. \u2019s (2019) framework on  socio-phonetic desi gn cues considers how social cue s can be \nderived from voice. In their framework , voice is viewed as  a combination  of so cial and physical \nproperties. T hey argue  that curr ent voice systems a re homoge nised in ter ms of voice output (i.e. , \ngender, acce nt), as well as that  voices  and re lated cues  shape user pe rceptions and e xperiences. \nHowever,  non-huma n soun ding voices are not considered  as part of their paper . Murad, Muntean u, \nCowan and Clark  (2019 ) offer design guidelines for speech interaction by reviewing  a set of existing \nguideli nes for  user interf aces an d extending thes e, yet only in  considerati on of mobile voice interfaces. \nTaking a contextual perspective, another  framework only cons iders emb odied  voice interfa ces \n(Cambre a nd Kulk arni, 2019) . While  a few  voice-based agent  class ification s already exist , establis hed \ncategorisati ons in the fiel d of IS  view voice as one  of several  modalities  without  providing a m ore \nrefined differentiation of  individual  auditory cues. As part of their taxonomy  on pedagog ical agen ts, \nWellnhammer, D olata, Steigler and Schwabe  (2020) illustrate that  agent voice can diff er among a \nclassic  text-to-speech  engine  (TTS), a modern TTS engine, and a human voice . Beyond  the type of \nvoice , however, no further modifications of auditory cues  are discussed .", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "bfa1faf9-e0d7-4773-ad67-24844e8b97e3": {"__data__": {"id_": "bfa1faf9-e0d7-4773-ad67-24844e8b97e3", "embedding": null, "metadata": {"page_label": "4", "file_name": "paper.pdf", "file_path": "eigene/paper.pdf", "creation_date": "2023-11-09", "last_modified_date": "2023-10-20", "last_accessed_date": "2023-11-10"}, "excluded_embed_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "0aa1876e-2be2-438e-884f-77f133c1b9be", "node_type": "4", "metadata": {"page_label": "4", "file_name": "paper.pdf", "file_path": "eigene/paper.pdf", "creation_date": "2023-11-09", "last_modified_date": "2023-10-20", "last_accessed_date": "2023-11-10"}, "hash": "ff61c384932e85b2cf6c2c32084a429ef9f66a8a4b6aa869bdf6396ec21c5753"}, "2": {"node_id": "e9b7a385-56a6-47a9-b30a-60712e34e4b1", "node_type": "1", "metadata": {"page_label": "4", "file_name": "paper.pdf", "file_path": "eigene/paper.pdf", "creation_date": "2023-11-09", "last_modified_date": "2023-10-20", "last_accessed_date": "2023-11-10"}, "hash": "1b2e28eeeaed9b5057fa2181a521700d4d3e84e18ca0958d2dd63173425c615f"}}, "hash": "3fd69c456f81628ec1140f654ad34f34d42ba56f21de483042f9b174b8219d24", "text": "Feine , Gnewuch, Morana and \nMaedche 's (2019)  taxonom y on conv ersati onal agents  provides a more nuanced understandin g of \ndifferent voice -relevant cue  categorie s, such as a di fferentiation between voi ce qualities  (i.e., volume, \npitch) and vocalizations  (i.e., laughin g). Nevertheles s, their taxo nomy does not include a revi ew of \nrelated effects on user s, which makes i t difficult to  derive an  understand ing of related interaction  \ndesign  implications . \nOvera ll, previous c ategor isation s exhibit several gaps. As me ntioned by Cambre an d Kulkarn i (2019), \nexisting re views lack a mapping  of VA-relevant voice cu es against user outcomes and their existing \nexploration i n research context s. To overcome gaps in existing literature , our study  suggests the \ndevelopmen t of a categorisation of VA-relevant  design cues 1) that covers auditor y cues prevalent and \nintroduced t hrough the de ployment of synthetic  voices, 2) that considers differen t types of VAs, 3) and \nthat derives  preva lent design strategies for VAs to be considered i n the futu re.  \n3 Metho dology  \nThe following section  lays out  the ch osen met hodology  to review existi ng research on auditory  cues \nreleva nt to voic e assistan ts. First, a sys temat ic literature rev iew (SLR) on voice-based conversational  \nagents  was conduct ed, following  established guidelines  (Webster & Watson, 2002) . An over view of \nthe literature  search  strategy is given in Figure 1. \nElectronic copy available at: https://ssrn.com/abstract=3910609", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "006d47bc-6903-4d82-92ca-f5699b7f2a8b": {"__data__": {"id_": "006d47bc-6903-4d82-92ca-f5699b7f2a8b", "embedding": null, "metadata": {"page_label": "5", "file_name": "paper.pdf", "file_path": "eigene/paper.pdf", "creation_date": "2023-11-09", "last_modified_date": "2023-10-20", "last_accessed_date": "2023-11-10"}, "excluded_embed_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "fe3f9758-e169-41e7-a420-d2f454a7b3b1", "node_type": "4", "metadata": {"page_label": "5", "file_name": "paper.pdf", "file_path": "eigene/paper.pdf", "creation_date": "2023-11-09", "last_modified_date": "2023-10-20", "last_accessed_date": "2023-11-10"}, "hash": "9f9476953591ac7752b186fa188864d1990a2603eec941706d407ee66e9e4017"}, "3": {"node_id": "726b4a7f-3aad-4b77-ba49-4663303a08fb", "node_type": "1", "metadata": {"page_label": "5", "file_name": "paper.pdf", "file_path": "eigene/paper.pdf", "creation_date": "2023-11-09", "last_modified_date": "2023-10-20", "last_accessed_date": "2023-11-10"}, "hash": "3781967c692772b2d753ebe64347a032e56de98d8eab554e6485c1fde4b19df7"}}, "hash": "2986b86c1c22d139fa34e060e42ca89b4e9cc682d5e91b28efb5dc102a878934", "text": "Schmitt et al. / Voice Contemporary Frontier Int eraction De sign \nTwenty -Ninth European Conference on Information Systems (ECIS 2021), A Virtual AIS Conference.      4  \nFigure 1. Systematic  literatur e search a nd screen. \nSix databases were selected , namely E BSCOhost, IEEE , ACM , ScienceDirec t, ProQuest  and AISeL . \nTo accou nt for synonyms and varying conceptualization s of convers ational ag ents (e.g., persona l \nassistant , virtua l assistant , intera ctive art ificial agent, interfa ce, bot, assistive  robot) and vo cal cues \n(e.g., voice -activated, voice -based, speech -based, spoken langua ge, spoken dialogue, conversational) , \nan exploratory search in open access and spe cific subje ct datab ases was co nducted to build a search \nstring. An initial r eview of around 30 papers ensured that we ident ified most common definitions and \nsynonyms for  VAs. This was particula rly import ant as the prel iminary search exhibited great \nfragmentation a cross research  fields that connotated VAs differently. We stopped  looking at additional \npapers on ce conno tations for voice assistants  started to  reappear wi th additi onal papers si ghted. The  \nsearch string ((voice) AND ( agent  OR as sistant OR in terface OR dialog ue system OR bot )) was \nemploye d, resulti ng in 3,946  search results.  The databas e searche s were limited  to include abstracts, \ntitles, or keywo rds of re levant publications only. Publications w ere assessed wit h respect t o the \nfollowing inc lusion criteria: All publicatio ns need ed to 1) refer to a VA that could  either  be based on  a \ndeterministic  or pr obabilistic  model , 2) analyse  vocal  cues that had an effect on the user (perceptua l or \nbehavioural ), 3) be peer-reviewed and written in English , 4) be published  in a recognized academic  \njourna l or repu table conference  and 5) be publi shed in b etween 2 000 and 2020. After r emoving all \nduplicates, a  forward and backward sear ch was last ly con ducted to identify f urther  significant \ncontributions to voice -based IT sy stems, i.e., also ou tside of the chosen date  range. A total of 86 \npaper s are considered relev ant to this stu dy.  \nThe 86 selected  papers  were  analysed from a c oncept-centric pers pective. A conc ept matrix was \ncreat ed based on the analys is of literature search re sults (Webster & Watson, 2002; Wolfswinkel et al., \n2013) in order to derive distinct pe rspectives on voice cues in t he context of conversatio nal agents, \nwhich represent  the u nit of analysis of this study. For each select ed paper , core concept s, research \nmethod , units of ana lysis includin g relev ant indep ende nt, dependent variab les and their relatio nships, \nwere  identified . Moreover, contextu al criteria  such as the app lication do main and  task of the VA were \ncaptu red. Findings  were synthesised  into centr al research per spectives  that amoun t to vocal design \nfeatur es of VAs. Process steps were iteratively vali dated by discussion among  three researc hers to \nensure stable, valid a nd reproducible  results.  For the systematic review , intercoder reliabilit y was \nchecked by letting three authors code ten selected  papers base d on an initial coding schem e drafted by \nthe first author . Differences in coding  allowed to refine the coding scheme and come to  a common \nunderstanding of the research focus among the  author s. In a similar vein, a n initial  framework for \nrelevant voice  cues was created by th e first author.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "726b4a7f-3aad-4b77-ba49-4663303a08fb": {"__data__": {"id_": "726b4a7f-3aad-4b77-ba49-4663303a08fb", "embedding": null, "metadata": {"page_label": "5", "file_name": "paper.pdf", "file_path": "eigene/paper.pdf", "creation_date": "2023-11-09", "last_modified_date": "2023-10-20", "last_accessed_date": "2023-11-10"}, "excluded_embed_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "fe3f9758-e169-41e7-a420-d2f454a7b3b1", "node_type": "4", "metadata": {"page_label": "5", "file_name": "paper.pdf", "file_path": "eigene/paper.pdf", "creation_date": "2023-11-09", "last_modified_date": "2023-10-20", "last_accessed_date": "2023-11-10"}, "hash": "9f9476953591ac7752b186fa188864d1990a2603eec941706d407ee66e9e4017"}, "2": {"node_id": "006d47bc-6903-4d82-92ca-f5699b7f2a8b", "node_type": "1", "metadata": {"page_label": "5", "file_name": "paper.pdf", "file_path": "eigene/paper.pdf", "creation_date": "2023-11-09", "last_modified_date": "2023-10-20", "last_accessed_date": "2023-11-10"}, "hash": "2986b86c1c22d139fa34e060e42ca89b4e9cc682d5e91b28efb5dc102a878934"}}, "hash": "3781967c692772b2d753ebe64347a032e56de98d8eab554e6485c1fde4b19df7", "text": "In a s ubsequent step, the co-authors were as ked to \nplace a selected number of papers in this framework . The framework was iteratively refine d over three \nrounds until  authors agreed on t he classificat ion of individual paper s within the framework . The \nfeedback of all authors helped to improve the system atic review s ince they have numero us years of \nresearch experience in the fie ld of conversational  agents. Neverthe less, a subjective bi as cannot be  \nfully exc luded as the process  of deriving concepts requires indiv idual judgement . \nElectronic copy available at: https://ssrn.com/abstract=3910609", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "51431c2b-5e97-4d60-8ca2-d67984a06cb5": {"__data__": {"id_": "51431c2b-5e97-4d60-8ca2-d67984a06cb5", "embedding": null, "metadata": {"page_label": "6", "file_name": "paper.pdf", "file_path": "eigene/paper.pdf", "creation_date": "2023-11-09", "last_modified_date": "2023-10-20", "last_accessed_date": "2023-11-10"}, "excluded_embed_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e2152268-5410-4733-b6a1-cfa6c355e475", "node_type": "4", "metadata": {"page_label": "6", "file_name": "paper.pdf", "file_path": "eigene/paper.pdf", "creation_date": "2023-11-09", "last_modified_date": "2023-10-20", "last_accessed_date": "2023-11-10"}, "hash": "eaa198ec0e25d2b44fbb1a0d20ea2ee79fae1607fa728b74e534c3366f444359"}, "3": {"node_id": "e98237d3-8b3d-402b-965e-de059acd5ff5", "node_type": "1", "metadata": {"page_label": "6", "file_name": "paper.pdf", "file_path": "eigene/paper.pdf", "creation_date": "2023-11-09", "last_modified_date": "2023-10-20", "last_accessed_date": "2023-11-10"}, "hash": "2b3b2db4a72c2dfa61a9b194f2fa7c35017e476de7b758392236896900494244"}}, "hash": "8cd5e6049ffd0dbe0f49434f9696a16e75266943f6165689b07b37a3585432ca", "text": "Schmitt et al. / Voice Contemporary Frontier Int eraction De sign \nTwenty -Ninth European Conference on Information Systems (ECIS 2021), A Virtual AIS Conference.      5 4 Results  \nBuilding up on the systema tic literature review, the  following sub sections address th is study \u2019s research \nquestion  by first understa nding voice  as a type of modality and by subsequentl y exploring i ndividua l \nvoice cues relevant to the design of VAs. Before  doing so, an overvi ew of the selected literature is \ngiven, touching up on temporal and contextua l research  trends, as well as prevalent domains  covered.  \nOver the analyse d time peri od rangi ng fr om 2 000 to 2 020, the numb er of publications has been \ngrowing  steadily . A steep increase in rel evant publications can be detecte d from 2018 onwards , with \nhalf of all reviewed studies being publ ished since then.  These findings  coincid e with the recent market  \npenetration  of smart spe akers such as Amazon Alexa  and Google Assist ant (Smith, 2020) . This \ndevelopment  supports our init ial assumptions that res earch on VAs is an emerging and to b e fur ther \ndevelop ed research field.  The commercial availability of VAs for personal use also becomes apparent \nwhen revie wing  domains cove red. More than a fourth (28%) of al l included  studies  focus on genera l \npurpose VAs, which were mainly explored in the context of daily activi ties and tasks. A numbe r of \npublicat ions have studied  the implic ations and suitabilit y of VAs for spe cific target grou ps, partia lly as \npart of  ethnographic home studi es, includ ing children  (Aeschlimann et al., 2020) , elderly  (Stra\u00dfmann \net al., 2020)  or disabled  (Masina et al., 2020) . However,  a few studi es did not further specify  the usage \ncontext or analysed  voice cues in simulated  task settin gs and lab environments . Based on nine \narchetypical purposes of conversational agents by Scarpellini and Lim (2020), we further identified the \nrespective role of each  VA reviewed. While general purpose, standalone VAs used in home settings \nmostly assist with  or perform pragma tic tasks , VAs in the Education, Healthcare , and Service domains \nexhibit a focus on the need to be reliable and explicative, as well as co nsider user s\u2019 preferences . \n   \nDomain  Task  \nAutom otive (11) Assist (4), Bo nd (1), Control (2), Educate (1), Entertain (1), Guide (1), Inform (1)  \nCollaboration (2)  Guide (2)  \nE-Commerce (11)  Advise (8), Inform (3)  \nEducation (12)  Coach ( 1), Educate ( 5), Entertain (2), Inform ( 4) \nGeneral (25) Advise (1), Assist (9), Control (2),  Entertain (1), Guide (5), Inform (2), Unspecified (5)  \nHealthcare (11)  Advise (4), Assist (1), Coach (2), Contr ol (1), Guide (2), Inform (1)  \nLeisure (5)  Assist (1), Bond (1), Entertain (1), Guide (1), Inform (1)  \nTable 2. Publi catio ns by domain and respective task. \n \nThe systematic literature review revealed two main perspec tives on the current understandi ng and \nexploration of VAs and their impact on user perceptions and behavioural  interaction.  First, l iterat ure \non interface modalities (e.g., text or visu al cue s) view voice  on a meta -level and  as a design strate gy \nnext to other types of modalities.  A second stream of literat ure explores individual vocal cues, such as \ngender o r volume of v oice.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "e98237d3-8b3d-402b-965e-de059acd5ff5": {"__data__": {"id_": "e98237d3-8b3d-402b-965e-de059acd5ff5", "embedding": null, "metadata": {"page_label": "6", "file_name": "paper.pdf", "file_path": "eigene/paper.pdf", "creation_date": "2023-11-09", "last_modified_date": "2023-10-20", "last_accessed_date": "2023-11-10"}, "excluded_embed_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e2152268-5410-4733-b6a1-cfa6c355e475", "node_type": "4", "metadata": {"page_label": "6", "file_name": "paper.pdf", "file_path": "eigene/paper.pdf", "creation_date": "2023-11-09", "last_modified_date": "2023-10-20", "last_accessed_date": "2023-11-10"}, "hash": "eaa198ec0e25d2b44fbb1a0d20ea2ee79fae1607fa728b74e534c3366f444359"}, "2": {"node_id": "51431c2b-5e97-4d60-8ca2-d67984a06cb5", "node_type": "1", "metadata": {"page_label": "6", "file_name": "paper.pdf", "file_path": "eigene/paper.pdf", "creation_date": "2023-11-09", "last_modified_date": "2023-10-20", "last_accessed_date": "2023-11-10"}, "hash": "8cd5e6049ffd0dbe0f49434f9696a16e75266943f6165689b07b37a3585432ca"}}, "hash": "2b3b2db4a72c2dfa61a9b194f2fa7c35017e476de7b758392236896900494244", "text": "W hile our analys is pred ominantly  focuses on  the current knowled ge base \nof auditory cues relevant to VAs, we see the first  literatu re stream as an important entry p oint to our \nanalysis. More spe cifically, by illustrating the variability in user and in teracti on outcomes for differ ent \ntasks and combinations of modality -mediated in teraction,  relevant strategies for VA d esign  and cri tical \naspects to consider  in the sub seque nt analysis can be identified.  \n4.1 Voice in Intermodal Comparison  \nIn what technological contex ts does t he inclusion or the isolated deployment of voice enha nce or eve n \noutperform o ther forms of VA interaction? One them e that emerged from  the lit erature was a  \ncomp arison of dif ferent modalities and on how  the presence (or ab sence) of  voice affected p erceptions \nElectronic copy available at: https://ssrn.com/abstract=3910609", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "a8bc167f-b8e3-4e00-9a95-de487a6e88aa": {"__data__": {"id_": "a8bc167f-b8e3-4e00-9a95-de487a6e88aa", "embedding": null, "metadata": {"page_label": "7", "file_name": "paper.pdf", "file_path": "eigene/paper.pdf", "creation_date": "2023-11-09", "last_modified_date": "2023-10-20", "last_accessed_date": "2023-11-10"}, "excluded_embed_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "17656815-0671-4f39-8e55-84ef48e7fce8", "node_type": "4", "metadata": {"page_label": "7", "file_name": "paper.pdf", "file_path": "eigene/paper.pdf", "creation_date": "2023-11-09", "last_modified_date": "2023-10-20", "last_accessed_date": "2023-11-10"}, "hash": "448e601f0c7b09b2b543b0fba5aea7f6de6a81483c43be161bb5c4370dacf867"}, "3": {"node_id": "b170b30b-8df7-4d10-bd7d-2a0930d088d7", "node_type": "1", "metadata": {"page_label": "7", "file_name": "paper.pdf", "file_path": "eigene/paper.pdf", "creation_date": "2023-11-09", "last_modified_date": "2023-10-20", "last_accessed_date": "2023-11-10"}, "hash": "f6bc875ecd6f2657c68a5aeafc50a7426d18a358bce2fb8288c367fa75b262af"}}, "hash": "453a4fc164c6a626088fa198c95fc534ed3f390c6a573023658f1f4462c9885d", "text": "Schmitt et al. / Voice Contemporary Frontier Int eraction De sign \nTwenty -Ninth European Conference on Information Systems (ECIS 2021), A Virtual AIS Conference.      6 of, respon ses to and interaction with a  (multimoda l) agent . Hess, Fuller and Campbell \u2019s (2009) study \non a websit e\u2019s reco mmendation  agent found that including a  voice featu re next to  feature s of text \nalone or text and human animat ion, led to grea ter perceptions  of social  presence  and henc e trust. In \nanother retaili ng experime nt where the  conver sational ag ent supporte d informati on r equests by \nshoppers, a t ouch-based in terface led to h igher user engagement with the platform as compared to an \ninterf ace that additi onall y deploy ed voice. In contrast, the voice -plus-touch interface weakened the \nrelati onship betwe en platform  engagement and br and trust. The author s argue  that the additional v oice \nfeature makes the interfa ce unnecessarily  complex  for the t ask at hand  (Pagani et al., 2019) . \nA numb er of studies have shown that voice m odalities on their o wn perform worse tha n other \nmodalities  of text, robotic face and bod y movements (Schl\u00f6gl et a l., 2013 ; Sh i et al., 2018) . \nChidambaram , Chiang and Mutlu  (2012) find that, while both non verbal  bodil y and vocal cues lea d to \nhigh user co mpliance w ith a robot \u2019s sugges tions, bodil y cues of the robot are percei ved as more \npersuasiv e than vocal cu es. In a stu dy on l earning outcom es, a fully embod ied a gent inc luding gaze \nand gesture ou tperformed a voice -only agent (Lusk and Atkinson, 2007) . An experimen t with the \nhealth and nutrition assi stant GRETA demons trated that GRETA interacti ng in the absence of t ext was \nperceived a s less likeab le. Howeve r, in the  voice-only interaction , users  could more easily mem orize \nthe information GR ETA pr ovided and t rusted t he robot more  (Berry et al., 2005) . In a si milar vein , in \na settin g of foreign langu age lea rning, the a gent\u2019s voice, operating here as an ed ucational in struct or, \ncontributed more positively  to learning outcome th an agent m ovem ent, ges tures and  pointing (Carlotto \n& Jaques, 2016) .  \nIt appea rs that the deployment  of voic e-based assista nts and the re spective effect on user and \ninterac tion must  be vi ewed embedde d in the use context and task  at hand , which is touche d upon more \nelaborately later in this pape r. In line with this reasoni ng, Cho, M olina and W ang (2019) found  that in \nthe conte xt of utilita rian tasks, voice leads to more positiv e attitudes towards the agent , which  is \nmediate d by perce ived hum an likeness o f the agent. Participants of their st udy were as ked t o interact \nwith Microsoft \u2019s voice agen t Cortana involvi ng two t ypes of ta sks, namely o ne utilitarian  and one \nhedoni c. As part  of a mixed factorial experiment, modality  (text versu s voice) and d evice ( mobile \nversus laptop) represent two within-subject facto rs of their study . Whether an d how the n ature o f task s \ninteracts with t he influenc e of vo ice is still  relativ ely unexplor ed in the con text of  conversatio nal \nagents. Interest ingly , Qiu an d Benbasa t (2005) explored the effect of differen t modalitie s in a customer \nservice context where the agent supported users with t heir product recomme ndations .", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "b170b30b-8df7-4d10-bd7d-2a0930d088d7": {"__data__": {"id_": "b170b30b-8df7-4d10-bd7d-2a0930d088d7", "embedding": null, "metadata": {"page_label": "7", "file_name": "paper.pdf", "file_path": "eigene/paper.pdf", "creation_date": "2023-11-09", "last_modified_date": "2023-10-20", "last_accessed_date": "2023-11-10"}, "excluded_embed_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "17656815-0671-4f39-8e55-84ef48e7fce8", "node_type": "4", "metadata": {"page_label": "7", "file_name": "paper.pdf", "file_path": "eigene/paper.pdf", "creation_date": "2023-11-09", "last_modified_date": "2023-10-20", "last_accessed_date": "2023-11-10"}, "hash": "448e601f0c7b09b2b543b0fba5aea7f6de6a81483c43be161bb5c4370dacf867"}, "2": {"node_id": "a8bc167f-b8e3-4e00-9a95-de487a6e88aa", "node_type": "1", "metadata": {"page_label": "7", "file_name": "paper.pdf", "file_path": "eigene/paper.pdf", "creation_date": "2023-11-09", "last_modified_date": "2023-10-20", "last_accessed_date": "2023-11-10"}, "hash": "453a4fc164c6a626088fa198c95fc534ed3f390c6a573023658f1f4462c9885d"}}, "hash": "f6bc875ecd6f2657c68a5aeafc50a7426d18a358bce2fb8288c367fa75b262af", "text": "Among  the six \ncondition s combining t he modalities text, v oice and animatio n, the text-to-speech voice of the agent  \ninfluenced  emotional trust,  as well as  cognitive trust  in the  agent \u2019s competence . \nUltimately, v oice is one of several interactio n modalities , depending on a number of d ifferent \nvariable s, includi ng contex t, user and task at ha nd. The c hoice fo r inclusion of voice or v oice-only also \nseems t o be a questio n of habitude and expe rience : In an ethnographi c study  with elderly using \npersonal assist ants at home,  an affin ity towar ds text-based com munic ation  could be  identified (Schl\u00f6gl  \net al., 2013) . In the past , voice a s a design  element has rather  been  an additional or complementary \nfeature to existing i nterfaces and informati on system s. Nowaday s, an increased deployment of v oice as \na stand -alone and di sembodied feature  can be expe rienced. With t he prevale nce of voice-only \nconversational ag ents in our personal homes, business settings, and se rvice frontli nes, this study ta kes \na closer l ook at ind ividual cues relevant to voice  design.   \n4.2 Voice Cue s and Voic e Design   \nWhat concrete  features must re searchers and designers consid er when d esigning v oice-based \ninterfaces ? More so,  how should such featu res be modified when designing for a  specifi c context o r \ninteraction ? In a seco nd step  of this p aper\u2019s analysis, w e overcome the shortcomings of previously \ntouched upon  categorisation s and taxonomies  by incl uding voice -related findings on  user impact . \nMore specificall y, our aim is to exa mine the ro le of individ ual voice cues on user perce ption and \nbehavioural  outcomes . We consider aud itory cues of VAs without limit ing the considered lite rature to \na certain device, task or  embodimen t of the agent. A combination of sev eral vocal cues by Feine e t al. \nElectronic copy available at: https://ssrn.com/abstract=3910609", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "13db9588-5abd-4d5a-8f95-69846d49d770": {"__data__": {"id_": "13db9588-5abd-4d5a-8f95-69846d49d770", "embedding": null, "metadata": {"page_label": "8", "file_name": "paper.pdf", "file_path": "eigene/paper.pdf", "creation_date": "2023-11-09", "last_modified_date": "2023-10-20", "last_accessed_date": "2023-11-10"}, "excluded_embed_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "32288d9e-325e-4485-9220-ed86c5897834", "node_type": "4", "metadata": {"page_label": "8", "file_name": "paper.pdf", "file_path": "eigene/paper.pdf", "creation_date": "2023-11-09", "last_modified_date": "2023-10-20", "last_accessed_date": "2023-11-10"}, "hash": "77a8aa540044fc49437e9b534e7a2492f4da2a65d66af226dc6fe0888ec316a6"}, "3": {"node_id": "d5dd142a-4ecc-43ea-bb30-960fb0e15d2d", "node_type": "1", "metadata": {"page_label": "8", "file_name": "paper.pdf", "file_path": "eigene/paper.pdf", "creation_date": "2023-11-09", "last_modified_date": "2023-10-20", "last_accessed_date": "2023-11-10"}, "hash": "20a6faadfacf3e1803bc73625db43470186f1f574974213f2af53d5606769b4a"}}, "hash": "e1fb04a44da86176f1f1e339a8adc7edd8f5218009d33c1b1a519cb8b6198ad5", "text": "Schmitt et al. / Voice Contemporary Frontier Int eraction De sign \nTwenty -Ninth European Conference on Information Systems (ECIS 2021), A Virtual AIS Conference.      7 (2019)  is taken as a point of departure  for organizing a categorisation  of vocal cues  and their effect on  \nuser perception and be haviour , which includes auditory voice qualities and voc alization s, as well a s \nchrone mics. While Feine et al. \u2019s (2019)  taxonomy  focuses on differe nt types of conversational  agents , \ntheir considera tion of a more nu anced variation  in vocal cues  as compared t o other taxonomies in the \nfield of IS , is deemed suitable fo r our s tudy. As pres ented in Table 2,  our categori sation of auditory \nfeatures is one w ay to  view  the analysis of  different t ypes of voic e-relevant design cues, yet th e \ncategorie s are not in tended to be collectivel y exhau stive. Rather, the categori sation focuses only on \nvoice cues that  have be en explicitly  explore d and manipula ted in existin g research on VAs.  \n \nRate of \nSpeech Number of words \n per minute / second \u2191Credibility, \u2191Trust,\n \u2191Acceptance \u2191Cooperation Rate book reviews,\n control blood\n measurement Nass & Lee (2001), Tay, Jung \n & Park (2014), Tielman et al. \n (2014)\n Fundamental\n Frequency (F0) and\n Formant Frequency\n (Df) \n Spectral Centroid\n Response Latency\n Voice Breaks\nFilled \nPausesBackchannel / Fillers \u2191Naturalness  \u2191Progressivity Assist in setting a\n quiz, chat about the\n news Fischer et al. (2019),\n Marge et al. (2010)\n SSML or TTS \n (as compared to\n human voice)\u2191 Trust, \u2191Preference, \n\u2193 Enjoyment, \n\u2193Naturalness,\n\u2193 Social presence \u2191Learning,  \u2193Learning\n \u2191Engagement, \u2191Training\n efficiency, \u2193Confirmation\n to suggestions, \u2191Physical\n distance to robot Educate & test on\n subject material, \n advise on product\n choice Craig & Schroeder (2017), \n Gravano & Hirschberg (2011), \n Mayer & DaPra (2012), Marge et \n al. (2010), McGinn & Torre \n (2019), Qui & Benbasat (2005), \n Qui & Benbasat (2009), \n Tamagawa et al. (2011), Walters \n et al. (2008)\n Gender\n (matched to user) \u2191Trust, \u2191Comprehension,\n \u2191Liking, \u2191Perceived ease\n of use, \u2191Perceived\n usefulness Robot Appearance\n Selection Answer general \n purpose requests, \n practice driving skills  Chang, Lu & Yang (2018), \n Cowan et al. (2016), Ernst & \n Herm-Stapelberg (2020), Lee, \n Ratan & Park (2019), Louwerse \n et al. (2005), McGinn & Torre\n (2019)\n Accent \n (matched to user) \u2191Naturalness,\n \u2191Preference, \u2191Trust Collaborate on \n picture taskCowan et al. (2016), \nTamagawa et al. (2011)\nNote: User Perceptions and Interaction Outcome findings associated with presence or an increase of voice cue at hand. \nE.g. Marge et al. (2010) found that the use of filled pauses increased perceived naturalness of interactionVoice \nCueAcoustic \nOperationalization\n \u2191Negotiation,\n \u2191Cooperation\n \u2191CooperationSelected Tasks \nof VA\n Negotiate price \n offers, play a quiz\n Guide security check\n for travel, advise\n money investment Amplitude [dB]\n \u2193Speech CollisionsUser Perception of VAInteraction Outcomes\n with VA\n Funakoshi et al.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "d5dd142a-4ecc-43ea-bb30-960fb0e15d2d": {"__data__": {"id_": "d5dd142a-4ecc-43ea-bb30-960fb0e15d2d", "embedding": null, "metadata": {"page_label": "8", "file_name": "paper.pdf", "file_path": "eigene/paper.pdf", "creation_date": "2023-11-09", "last_modified_date": "2023-10-20", "last_accessed_date": "2023-11-10"}, "excluded_embed_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "32288d9e-325e-4485-9220-ed86c5897834", "node_type": "4", "metadata": {"page_label": "8", "file_name": "paper.pdf", "file_path": "eigene/paper.pdf", "creation_date": "2023-11-09", "last_modified_date": "2023-10-20", "last_accessed_date": "2023-11-10"}, "hash": "77a8aa540044fc49437e9b534e7a2492f4da2a65d66af226dc6fe0888ec316a6"}, "2": {"node_id": "13db9588-5abd-4d5a-8f95-69846d49d770", "node_type": "1", "metadata": {"page_label": "8", "file_name": "paper.pdf", "file_path": "eigene/paper.pdf", "creation_date": "2023-11-09", "last_modified_date": "2023-10-20", "last_accessed_date": "2023-11-10"}, "hash": "e1fb04a44da86176f1f1e339a8adc7edd8f5218009d33c1b1a519cb8b6198ad5"}}, "hash": "20a6faadfacf3e1803bc73625db43470186f1f574974213f2af53d5606769b4a", "text": "(2010) \u2191Trust, \u2193Trust,\n \u2191Credibility, \u2191Robot\n Appeal, \u2191Acceptance,\n \u2191Enjoyment, \u2191Interaction\n Quality, VA Gender &\n Emotion  Elkins & Derrick (2013), McGinn \n & Torre (2019), Nass & Lee \n (2001), Tay, Jung & Park (2014), \n Torre et al. (2020), Walters et al. \n (2008) \u2191Fairness, \u2191Dominance\n \u2191Credibility, \u2191Trust\n \u2191Acceptance Mania et al. (2020), Nass & Lee \n (2001), Tay, Jung & Park (2014), \n Tielman et al. (2014)Selected Research\n Guide hotel room\n reservation\nSynthetic \nVoicePausing / \nLatencyAuditory Cues\nChronemis Voice Qualities\nPitchLoudnessVA-Specific Cues\n \nTable 3. Relevant voice cues along user perceptions  and interaction outcomes.  \n \nAuditory  cues refer to feature s of voice  and utterance which may not be enco ded by phonet ic \nsegments, gram mar or choice o f vocabulary . They  include  the lou dness or volume, th e speech ra te, \nand pitch  of a voice . Each  of these cues can be ac oustically operationali zed through  different \nmeas ures. Loudness , for instance, c an be measured thro ugh the ampl itude as measured in  decibels , \nwhile speech rate  can be adapte d by modif ying the nu mber of w ords spok en per minute. Tweaki ng \nproso dic v oice cues i s important as they can influence users \u2019 percept ions of a VA with regard to its \ncredi bility, appeal or accept ance (Nass & Lee, 2001; Sutton, 2020; Tay et al., 2014) . Second, such \nmodi fication  can evo ke associations of personality (i.e., gender) or the emotional sta te of a voice.  \nTorre, Goslin and White  (2020)  ident ified a higher forman t frequency ( measured as the number of \nElectronic copy available at: https://ssrn.com/abstract=3910609", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "bee1e4ce-2c73-4d96-ba76-be60444c7aa9": {"__data__": {"id_": "bee1e4ce-2c73-4d96-ba76-be60444c7aa9", "embedding": null, "metadata": {"page_label": "9", "file_name": "paper.pdf", "file_path": "eigene/paper.pdf", "creation_date": "2023-11-09", "last_modified_date": "2023-10-20", "last_accessed_date": "2023-11-10"}, "excluded_embed_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4a4e89b4-63f8-46a9-8041-e539607181b3", "node_type": "4", "metadata": {"page_label": "9", "file_name": "paper.pdf", "file_path": "eigene/paper.pdf", "creation_date": "2023-11-09", "last_modified_date": "2023-10-20", "last_accessed_date": "2023-11-10"}, "hash": "27e16056238cc63602e56a8f835cbc699cf7b727bf880af91f62d6bdc264c581"}, "3": {"node_id": "64342766-ba3c-4302-9c57-b8ac3418af80", "node_type": "1", "metadata": {"page_label": "9", "file_name": "paper.pdf", "file_path": "eigene/paper.pdf", "creation_date": "2023-11-09", "last_modified_date": "2023-10-20", "last_accessed_date": "2023-11-10"}, "hash": "e4e4c7c6b0856797709de5c7b589d392911c016b9092e092b89912af3bad9dfb"}}, "hash": "4d5ca1f718055bc141d7c53a480083c1cc5652e3df52bc217ec76609fd6c70dc", "text": "Schmitt et al. / Voice Contemporary Frontier Int eraction De sign \nTwenty -Ninth European Conference on Information Systems (ECIS 2021), A Virtual AIS Conference.      8 cycles of a soundwave per second of vibratin g air particles ) and higher spectra l centro id (which r efers \nto how energy is distribu ted ac ross different frequency b ands) as automated measures of pitc h and \nacoustic  correla tes o f a smiling voice. Deploying th is voic e-based robot in a n investment task  found an \nincrease in tru st resulting from a higher pitch . Interestingly, an increased pitch has been associated \nwith a dec rease in tr ust in an interview settin g with an embodied conversa tional agen t (Elkins & \nDerrick, 2013) .  \nAs part of auditory  cues, chrone mics are timing-related cues in communication and hence related to \nturn-taking  (Feine et al. , 201 9). Turn-taking is c rucial to the design of voice -based intera ctions si nce it \ndefines t he flow and p rogressivity of an in teracti on (Fisch er et al., 2019) . Gravano and Hi rschberg  \n(2011)  identified seven turn -yielding cues from hum an-to-human  interact ion which are also \nautomatica lly computable. Backchannels , for instan ce, ar e short expre ssions such as \u201cuh-hu\u201d or \u201cmm-\nhm\u201d which convey that the list ener is paying att ention and encoura ge the s peaker to continue talking. \nResponse latency is a nother turn-taking metric,  often measur ed as  reply speed or  the time in between \ntwo conver sation turns.  Funakos hi et al.  (2010) found t hat non -humanlike turn -taking (measured as \nslower than av erage reply speed in milliseconds ) resulted in less speech co llisio ns and hence  prove s as \nsuitable for an interaction with a sp oken dialogue  system . Contrary to F unakoshi et  al.\u2019s (201 0) study  \nin the  context of a hotel room reservation task , Marge, Miranda, Black an d Rudnicky  (2010)  explore \nthe effect of voice b reaks and filled pauses  on perceived  naturalne ss in a social conversation  with a \nspoken d ialogue system. They f ind that  the in clusion of such pauses increase s the perce ived \nnaturalness of the interaction a nd can compen sate for a lack of logical flow.  \nA second  dimen sion t itled VA-specific cues relate s to cues that have been identified  as eithe r newly \narising features in the design of VAs or predo mina nt featu res whic h cannot be further broke n down \ninto any of the previousl y mentio ned cues and which can be directly manipulated with a synthesized or \nhuman  recorded voice . Synthesi zed voice , as opposed to record ed human voi ce, en ables VAs to be \nmatched to certain tasks and  contexts. Plat forms off er several  synthesiz ed text-to-speech (TTS ) voice \nlibrar ies and Speech Synthesis Marku p Langua ge (SS ML) for fur ther person alization  (Branham & \nRoy, 2019) . Information  and conte nt can b e fed through written dialo gue or spoken corpora  \n(Wellnhammer et al., 202 0). While t he deployment of a synt hetic voice  as compared to recorded \nhuman voice  has bee n associa ted with less na turalness and social pre sence, numerous studies have \nshown a  signifi cant pos itive effect on trus t and preference .", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "64342766-ba3c-4302-9c57-b8ac3418af80": {"__data__": {"id_": "64342766-ba3c-4302-9c57-b8ac3418af80", "embedding": null, "metadata": {"page_label": "9", "file_name": "paper.pdf", "file_path": "eigene/paper.pdf", "creation_date": "2023-11-09", "last_modified_date": "2023-10-20", "last_accessed_date": "2023-11-10"}, "excluded_embed_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "4a4e89b4-63f8-46a9-8041-e539607181b3", "node_type": "4", "metadata": {"page_label": "9", "file_name": "paper.pdf", "file_path": "eigene/paper.pdf", "creation_date": "2023-11-09", "last_modified_date": "2023-10-20", "last_accessed_date": "2023-11-10"}, "hash": "27e16056238cc63602e56a8f835cbc699cf7b727bf880af91f62d6bdc264c581"}, "2": {"node_id": "bee1e4ce-2c73-4d96-ba76-be60444c7aa9", "node_type": "1", "metadata": {"page_label": "9", "file_name": "paper.pdf", "file_path": "eigene/paper.pdf", "creation_date": "2023-11-09", "last_modified_date": "2023-10-20", "last_accessed_date": "2023-11-10"}, "hash": "4d5ca1f718055bc141d7c53a480083c1cc5652e3df52bc217ec76609fd6c70dc"}}, "hash": "e4e4c7c6b0856797709de5c7b589d392911c016b9092e092b89912af3bad9dfb", "text": "More  so, in lear ning contex ts, a synthe tic \nvoice has proven to  positively aff ect learn ing transfer , training efficien cy and en gagement  (Craig and \nSchroeder, 2017 ; Komiak and Benbasat, 2003; Qiu  and Benbasa t, 2005; Tamagawa et  al., 2011 ).  \nBased on the analysis and mapping of voice  cues and related user outcomes, we identify  three \ndominant design themes  the reviewed literature. These findings are in line with the  two voice  desig n \nstrategies mentioned by Sutton, Foulkes, Kirk and Lawson  (2019), referring to  the individualisation of \nvoice output, and  context aware ness when d esigning  VAs. While the two design goals have been \nintroduced focusing on sociophonetic  aspects of VAs only , we extend these resp ective design \nstrategies  to all audit ory cues  identified  in this review . More so, we expand this previous work i n \nsuggesting a third relevant design st rategy,  namely  the imperson ation, or personification of VAs.  It is \nimportant to n ote here that  these out put strategies should not be v iewed as mut ually exclusive , \ncollecti vely exhaustive  nor ultimately desirable  for VA design . Rather , we see th ese three design  \nstrategies as a way to stru cture the prevalent themes found  in the reviewed literatur e and as t opics  \naround which previous , related  design  choices should be challenged.    \n5 Strategie s for Voice Design  \n5.1 Personificati on of Voice Ass istants  \nIn lin e with Media Equat ion Theor y, humans do not only  assign  personalities  to other  humans yet also \nto machine s (Reeves & Nass, 1996) . Our categori sation of voice cu es and related mappin g of user \npercep tions is congruent  with this theory,  demonstrati ng that b oth ind ividual voice  cues,  as well as the \ncombination of cues trans late into certain socia l cues and associations w ith the VA to create a \u201cvoice \nElectronic copy available at: https://ssrn.com/abstract=3910609", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "b6909f54-10d8-47bc-b176-324f77502f86": {"__data__": {"id_": "b6909f54-10d8-47bc-b176-324f77502f86", "embedding": null, "metadata": {"page_label": "10", "file_name": "paper.pdf", "file_path": "eigene/paper.pdf", "creation_date": "2023-11-09", "last_modified_date": "2023-10-20", "last_accessed_date": "2023-11-10"}, "excluded_embed_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "966bf940-e06a-4294-96b8-1c6f319525b0", "node_type": "4", "metadata": {"page_label": "10", "file_name": "paper.pdf", "file_path": "eigene/paper.pdf", "creation_date": "2023-11-09", "last_modified_date": "2023-10-20", "last_accessed_date": "2023-11-10"}, "hash": "036fc22187e04084a7f40791d85fbfb6a8bd88a38dfee19d1e5bdec83432f0c3"}, "3": {"node_id": "7ad45a9b-6e1c-4f39-96a5-3bc4b6e5e047", "node_type": "1", "metadata": {"page_label": "10", "file_name": "paper.pdf", "file_path": "eigene/paper.pdf", "creation_date": "2023-11-09", "last_modified_date": "2023-10-20", "last_accessed_date": "2023-11-10"}, "hash": "ac8deff0fe02d7a6d2cabc2d1f23e8cf37556f10a0ba844f3c68cc70f3c409f2"}}, "hash": "207bd42f19522ea418c85a9ff0c53f52a321f20f24b4a1e5e734c35457974c4e", "text": "Schmitt et al. / Voice Contemporary Frontier Int eraction De sign \nTwenty -Ninth European Conference on Information Systems (ECIS 2021), A Virtual AIS Conference.      9 personality \u201d. Aiming to establish  more natural interaction s, existing stud ies predominantly ass ess the \nhuman lik eness  of VAs. Last, the deployment  of a \u201cdefault\u201d voice has also been fo und as  a prevalent \nvoice design strategy and is critic ized by numerous  papers, pointing towards a lack  of consciou s and \nembedded voice design .  \nIn fact, V\u00f6lkel et al.  (2020 ) developed  a personality model  for speech -based conv ersationa l agents . \nBased on an ag gregated set of 349 a djecti ves, they develop ed a set of sy nonym cl usters to descr ibe the \npersonality of a VA. Their analysis was based on an  examination of  commercia lly available V As, \ninclu ding Google Assistant, Cortana  and Ale xa. More so, their study demonstrates how VAs are \nascri bed certa in personality traits  and that pe rceptions of ag ent perso nality ca n be shaped deliberately . \nInterestingly, they propo se an initial  set of VA personality  dimensions , arguing that existi ng model s to \ndescribe human personality, such as the  Big Five, are inadequate for VAs. The research  interest in   VA \nanthropomorphi sm become s apparen t by considering  the sh eer number of  studies explo ring the \nhumanness a nd naturalness of in dividual vo ice cues. Existing  research  has demonstrated th at the \nimitation of v oice cues foun d in human -to-human interaction, suc h as respon se latency, can i ncrease \nthe perceived naturalness o f VAs (Marge et al., 2 010). However, the investi gation o f individual \nacou stic measure s and their t weaking towards greater human  likeness should be f urther researched .  \nChang, Lu and Yang (2018) , who co mpared eight  synthetic voices differing in gender , pitch, s peech \nrate and in tonation, not only found that t he adoption of voice c haracteristics  to user preferences migh t \nperform better t han a \u201cone size  fits a ll\u201d approach, but that the a ssociation and comb ination of various \ncues created certa in personal ity trait associatio ns with the social robots . In a similar vein, Mania et al.  \n(2020) modified pitch, amplit ude an d intonatio n to create impressions o f vocal  dominance o f the VA . \nWhile  the modulation of v ocal cues was sufficient  in conveying different  degrees of do minance,  a \nrelated differ ence in ne gotiation action was only present for participan ts who perceive themsel ves as \ncompeti tive nego tiators. In an e-comm erce setting, Nass a nd Le e (2001)  manipulat ed voice with \nregard to four param eters (volume, fundamental  freque ncy, frequency  range, and spee d rate ) to creat e \nextroverted and i ntroverted  voice personali ties. The personality convey ed by the v oice had a m ain \neffect on use r percep tions: Participants wh o listened to the same book review ra ted those  differently \non liking and credibi lity based on hearing these reviews in different voi ces. To maximize liking and  \ntrust, the  authors  argue, voice cues should  be set i n such a way which creates a personality t hat is \nconsisten t with the user and the co ntent being presented , thus shaping intima te service interac tions. \nMod ifying a VA \u2019s extraversion  by twitchin g multiple voice  cues has been a  common meth od found in \nseveral studies, suc h as Tay et al. 's (2014)  exper iment in   healthcare , as wel l as Hess, F uller and \nCampbell  (2009) study on m ultimedia vividness.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "7ad45a9b-6e1c-4f39-96a5-3bc4b6e5e047": {"__data__": {"id_": "7ad45a9b-6e1c-4f39-96a5-3bc4b6e5e047", "embedding": null, "metadata": {"page_label": "10", "file_name": "paper.pdf", "file_path": "eigene/paper.pdf", "creation_date": "2023-11-09", "last_modified_date": "2023-10-20", "last_accessed_date": "2023-11-10"}, "excluded_embed_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "966bf940-e06a-4294-96b8-1c6f319525b0", "node_type": "4", "metadata": {"page_label": "10", "file_name": "paper.pdf", "file_path": "eigene/paper.pdf", "creation_date": "2023-11-09", "last_modified_date": "2023-10-20", "last_accessed_date": "2023-11-10"}, "hash": "036fc22187e04084a7f40791d85fbfb6a8bd88a38dfee19d1e5bdec83432f0c3"}, "2": {"node_id": "b6909f54-10d8-47bc-b176-324f77502f86", "node_type": "1", "metadata": {"page_label": "10", "file_name": "paper.pdf", "file_path": "eigene/paper.pdf", "creation_date": "2023-11-09", "last_modified_date": "2023-10-20", "last_accessed_date": "2023-11-10"}, "hash": "207bd42f19522ea418c85a9ff0c53f52a321f20f24b4a1e5e734c35457974c4e"}}, "hash": "ac8deff0fe02d7a6d2cabc2d1f23e8cf37556f10a0ba844f3c68cc70f3c409f2", "text": "The latter mo dified speed rate , frequency range , \npitch and vol ume to ev oke an e xtroverted or introverted a gent personality, showing that suc h \npersonality diff erence inf luences social presen ce and ul timately t rust in the  VA. In a more re cent \nstudy , increased  formant frequency and s pectral cen troid were used a s acoustic  correlates of a  smiling \nrobot vo ice (Torre, Goslin, et al., 2020) . Not only  did parti cipants rat e \u201csmilin g voices \u201d as happier but  \ntrusted those mo re.  \nA numb er of researchers  have critic ized a defaul t set o f voice parameters , as commonly c hosen by \ncomm ercial  interfaces (Cambre & Kulk arni, 2019; Torre et al., 2018) . By relying on  a predefined set \nof design pa rameters , a c ertain voice personalit y (mostly fe male) is deployed  by default  without \nconsideri ng the effect s and suitability of this voice character  for the u se case at hand . Hwang, O h, Lee \nand Lee  (2019)  have reviewed 1,602 responses from fi ve South Korean VAs, finding that fema le \nvoices exist as the only default voice set ting, ultimately enhancing stereotypical femin inity. In fact, t he \ndeploym ent of a female voice as the defau lt gender has been associated with increase d likeabili ty and \npreference  (Chang et al., 2018; Ernst & Herm -Stapelberg , 2020) . Technologoical companies are \nslowly starting t o offer several voice  options instead of a default female  voice , such as Apple\u2019s Siri  \n(Khaled, 2021) . Beyond  a default v oice, c omm ercia l devel opers are curre ntly exploring the vast  \nopportunities of voice modification  to deliver a natural  exper ience and facilitated interaction flow, \nincluding a \u201cOne-Breath  Test\u201d to gu ide speech ra te of sy ntheti c voice  or the design of pauses in -\nbetwe en a certa in nu mber of  words  (Branham and Roy , 2019 ).  \nElectronic copy available at: https://ssrn.com/abstract=3910609", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "3873ba07-bdea-4dd9-b60f-91e17a28e327": {"__data__": {"id_": "3873ba07-bdea-4dd9-b60f-91e17a28e327", "embedding": null, "metadata": {"page_label": "11", "file_name": "paper.pdf", "file_path": "eigene/paper.pdf", "creation_date": "2023-11-09", "last_modified_date": "2023-10-20", "last_accessed_date": "2023-11-10"}, "excluded_embed_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5343348a-9323-40cf-add1-f852bfe54ddc", "node_type": "4", "metadata": {"page_label": "11", "file_name": "paper.pdf", "file_path": "eigene/paper.pdf", "creation_date": "2023-11-09", "last_modified_date": "2023-10-20", "last_accessed_date": "2023-11-10"}, "hash": "402f8b39e2cbb3d75db6e715206dadcbeb32db4b53a7928a9a4f1858f2be8768"}, "3": {"node_id": "1ebd5849-4bfb-4479-899b-9be3b03c7f9f", "node_type": "1", "metadata": {"page_label": "11", "file_name": "paper.pdf", "file_path": "eigene/paper.pdf", "creation_date": "2023-11-09", "last_modified_date": "2023-10-20", "last_accessed_date": "2023-11-10"}, "hash": "2dc86bd710dd4af917246a18cef6613330fdd609612486f7bd44fb6260516b80"}}, "hash": "3348556f1bca51fdbb3687a15e4816d5aed4e68546cff3d0febee815def43e12", "text": "Schmitt et al. / Voice Contemporary Frontier Int eraction De sign \nTwenty -Ninth European Conference on Information Systems (ECIS 2021), A Virtual AIS Conference.      10 So far, little research has taken into consideration  adverse and unintended effects of personification, as \nwell as the context -sensitivity regarding the effectiveness of personification. Raising the human -\nlikeness of VAs may be bene ficial in rel ational interaction contexts , yet could impede application \ncontexts  that require anonymity, i.e.,  where sensitive data is shared. As proposed by the Uncanny \nValley,  the notion of an anthropomorphized, yet not fully human VA might evoke feelings of eeriness  \nas a high human -like appearance creates expectations which ar e not matched by the agent\u2019s \ncapabilities (Mori, MacDorman and Kageki, 2012). Anthropomorphizing VAs  through voice cues can \nlead to  frustration , especially if technical functionalities are n ot ensured or personification  is \nunexpected.  In a similar vein, researchers have criticized the impersonation of V As to an extent where \nusers no longer can distinguish the machine voice from a human voice, thus deceiving  the real identity \nof a VA (O\u2019Leary, 201 9). \n5.2 Individuali zation  of VA Interaction   \nThe previously raised inappropriaten ess of a defaul t design approach for VAs points towar ds another \nkey finding  of this study \u2019s review : The importance of tailoring a VA towards users and the ir individual  \npersonality , preferences and expectation s. In a real-world driving study, VA characters w ere trusted \nmore and rate d as signific antly more likea ble as compared  to a default voice character  that matched \nthe driver \u2019s personali ty (Braun et al., 2019) . Traits of the charac terize d VAs were ex pressed through \nboth choice of words and intona tion. Their stud y even shows tha t a m ismatc hed assis tant perso nality i s \nperceived as much less useful and sa tisfyi ng th an the defaul t VA. In a similar vein, matching system \naccent to users \u2019 natio nality has been studied in several  papers, exh ibiting increase d levels of \nnatura lness , prefe rence  and trust (Cowan et al., 2016; Tamagawa et al., 2011) . Zepf,  Gupta, Kr\u00e4mer \nand Minker  (2020)  explo re acoustical mimicry as a  strategy to improve empathy within automot ive \nuser interfaces and demonstrate a  positive effect on perceived emp athy, personalizatio n and \nnaturalness without af fecting the effici ency of the driving task . In a study by Nguye n, Ta and Prybutok  \n(2019) , femal e and male participa nts exhibi t differe nt factors that drive  attitudes toward VA use and \nadoption, i llustrating how g ender presents  an importan t user characteristic to consider  when desi gning \nfor system adoption and use. Adapting  towards us ers or even mirrorin g their demog raphics and \nbehaviour  has been explored  as a potent ial desi gn strategy fo r voice -based interactions. Eyssel et al.  \n(2012)  underline thi s hypothesis with their study,  finding that same -gender r obots are perceived m ore \npositive ly, ar e associate d with more psychologic al cl oseness  and are mor e stro ngly \nanthropo morphized.  Significa nt differences in perce ived pleasantness  and VA g ender preferen ce \nbetween male and female par ticipants have also been found in Obinali 's (2019)  study on information  \naccepta nce of voice output by Apple\u2019s Siri. Even in the context  of a TTS vo ice, matc hing the agent \nvoice \u2019s gender  towards the user at hand reduces cognit ive demands while driving and makes  the \ninterface more effective to  use (Truschin et al., 2014) .", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "1ebd5849-4bfb-4479-899b-9be3b03c7f9f": {"__data__": {"id_": "1ebd5849-4bfb-4479-899b-9be3b03c7f9f", "embedding": null, "metadata": {"page_label": "11", "file_name": "paper.pdf", "file_path": "eigene/paper.pdf", "creation_date": "2023-11-09", "last_modified_date": "2023-10-20", "last_accessed_date": "2023-11-10"}, "excluded_embed_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "5343348a-9323-40cf-add1-f852bfe54ddc", "node_type": "4", "metadata": {"page_label": "11", "file_name": "paper.pdf", "file_path": "eigene/paper.pdf", "creation_date": "2023-11-09", "last_modified_date": "2023-10-20", "last_accessed_date": "2023-11-10"}, "hash": "402f8b39e2cbb3d75db6e715206dadcbeb32db4b53a7928a9a4f1858f2be8768"}, "2": {"node_id": "3873ba07-bdea-4dd9-b60f-91e17a28e327", "node_type": "1", "metadata": {"page_label": "11", "file_name": "paper.pdf", "file_path": "eigene/paper.pdf", "creation_date": "2023-11-09", "last_modified_date": "2023-10-20", "last_accessed_date": "2023-11-10"}, "hash": "3348556f1bca51fdbb3687a15e4816d5aed4e68546cff3d0febee815def43e12"}}, "hash": "2dc86bd710dd4af917246a18cef6613330fdd609612486f7bd44fb6260516b80", "text": "Turning towards the  intelligibl e nature of state -of-the-art VAs, Lubold, Walker and  Pon-Barry  (2020)  \nfind tha t VAs adopt ing the ir pitch towa rds th e user, in combination  with social dialogue, ca n lead to \nhigher measu res of rapport. In commercial  cases, so-called tapering  is used as a strategy to ad opt the \namount of detail provided  in each interactio n and reduci ng it in line with the number of times a  servi ce \nhas been used  (Branham & Roy, 2019) . In addition, s peech rate a nd timeout pe riods hav e been \nconsidered as computable  voice feature s to be ta ilored  towards  certain target populations . As visually \nimpaired  people can usually process voice much faster , design papers have raised  the need for speec h \nrate to be adapt able. In a similar vein, elderly or disabled hum ans may take longer to formulate a \ncommand. He nce, su ch personal r equiremen ts trans late in to a need for longer timeout periods of VAs \n(Branham & Roy, 2019) . However,  the current rese arch field is lacking an em pirical explo ration of  the \neffects of individual design cue s of such commercially av ailable VAs (Knote et al., 2019 ). Beyond  \nqualitative docum ent r eviews of commercial design guidelines , little research can b e found on VA -\nspecific  cues.  \nAn individualized VA interaction  enables nove l opportunities for minorities, i.e., by adapting to the \nneeds of elderlies or tailorin g an interaction towards a spe cific accent  and thus reaching previously \nElectronic copy available at: https://ssrn.com/abstract=3910609", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "b9e919ab-5906-4fda-8a12-06e1feb115ff": {"__data__": {"id_": "b9e919ab-5906-4fda-8a12-06e1feb115ff", "embedding": null, "metadata": {"page_label": "12", "file_name": "paper.pdf", "file_path": "eigene/paper.pdf", "creation_date": "2023-11-09", "last_modified_date": "2023-10-20", "last_accessed_date": "2023-11-10"}, "excluded_embed_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "73b1df26-3a2f-40bc-876a-a267b3b5ded5", "node_type": "4", "metadata": {"page_label": "12", "file_name": "paper.pdf", "file_path": "eigene/paper.pdf", "creation_date": "2023-11-09", "last_modified_date": "2023-10-20", "last_accessed_date": "2023-11-10"}, "hash": "c22bff5b3086d9450070731b6b9af90112b2410916ac3050acc83599c552dbfd"}, "3": {"node_id": "a1bfaad4-78d3-4411-bc23-8a947fa7bceb", "node_type": "1", "metadata": {"page_label": "12", "file_name": "paper.pdf", "file_path": "eigene/paper.pdf", "creation_date": "2023-11-09", "last_modified_date": "2023-10-20", "last_accessed_date": "2023-11-10"}, "hash": "72553fb40ef752784169a8d50cb3116b6953ba23bac474d1428a612901193aec"}}, "hash": "3661d5f768029fb689743af0ea83cf2334cbf2a39fdfe8f9394ec84b248e12d3", "text": "Schmitt et al. / Voice Contemporary Frontier Int eraction De sign \nTwenty -Ninth European Conference on Information Systems (ECIS 2021), A Virtual AIS Conference.      11 neglect ed user group s (Sutton et al., 2019) . However, th is room for adaptation also leaves room for the \nmanifestation of s tereotypes , especially if the choice of design cues is based on stereotypical \nfunctiona lities  and human bias . For instance, the design of a VA \u2019s gende r has shown that a male voice \nis deploye d for security tasks  (Trovato et al., 2017) , wherea s standalone  VAs deploy ed for home use \nare female (Carpenter et al., 2009) . Beyond designing for increased  likability and adoption of VAs, \ndesigners should consider how certain des ign choices manif est or even en hance stereotypes.  \n5.3 Context  Awareness in VA Interaction  \nThe importance of designing  voice \u201cin relation to t he contexts in which it is us ed\u201d (Sutton et  al., 2019 , \np. 7) has been ra ised by a number of researchers. In 200 6 already , Mutlu et al. argued  that voice desi gn \nshoul d be viewed as a n interplay among user characteristics, device and context. Accordingly , the \nprevious two  desig n output strategies  cannot be view ed is olated from this third one, c ontext \nawareness. Relev ant aspe cts that h ave already been to uched  upon  with reg ard to contex tualization  \ninclude the consideration  of geogr aphical accent (Cowan et al., 2016; Tamagawa et al., 2011) , yet \nextend to social quali ties a voice is expected to embody  in a certain use  case. The previous discussion \naround adaption towar ds soci o-demographical fun ctionalities c an already be viewed as a form of \ncontextua lization. When tailoring voice towards an ac tivity or task , these expe cted qualities differ  \nfrom int eraction to interaction. In the following, we therefore refer to a specif ic task  or domain when \ntalking about con text. \nIn a study by Torre, Latupeirissa and McGinn  (2020) , participant s were a sked to match different \nvoices to images of robo ts. A predefined number of contexts consi sted of  a home, h ospital, restaura nt \nand school  setting each , and were a dded as background noise t o the robot voice. Interestingly, \ndifferent robot  images were c hosen for diff erent conte xts d espite other vocal featu res (naturalness, \ngender, accent) remaini ng the same . The findings i llustra te that the effect of indiv idual voice  cues \ninteract with t he co ntext of a n agent \u2019s deployment. Supporting this line of reasoning, a study  by Tay et \nal. (2014)  explored  the imp act of a VA \u2019s gender and pe rsonality in two different occup ational settin gs, \nnamely  a healt hcare and a secu rity scenario . The study  demonstrated that VA gender and personality \ndo not m onotonically influence user acceptance , yet rathe r interact with corresponding role ster eotypes \nof certain oc cupations in  their effect on use r acceptance  (Tay et a l., 2014). Sensitivity in voice design \nalso becomes rele vant when a VA is deployed in context s of high cognit ive load, such as a driving \nscenario. When  safety is of utmost imp ortance, creatin g high perceptions of VA trustworthiness or \nnaturalness might not be t he desired design goal. More s pecifically,  Wong, Brum by, Babu and \nKobayashi  (2019)  found that a more assertive VA alerti ng the driver  to dri ving even ts resul ted in \nfaster reaction times and  was perceived as mor e urgent .  \nTurning towards  commercial design el ements , Tabassum et al.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "a1bfaad4-78d3-4411-bc23-8a947fa7bceb": {"__data__": {"id_": "a1bfaad4-78d3-4411-bc23-8a947fa7bceb", "embedding": null, "metadata": {"page_label": "12", "file_name": "paper.pdf", "file_path": "eigene/paper.pdf", "creation_date": "2023-11-09", "last_modified_date": "2023-10-20", "last_accessed_date": "2023-11-10"}, "excluded_embed_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "73b1df26-3a2f-40bc-876a-a267b3b5ded5", "node_type": "4", "metadata": {"page_label": "12", "file_name": "paper.pdf", "file_path": "eigene/paper.pdf", "creation_date": "2023-11-09", "last_modified_date": "2023-10-20", "last_accessed_date": "2023-11-10"}, "hash": "c22bff5b3086d9450070731b6b9af90112b2410916ac3050acc83599c552dbfd"}, "2": {"node_id": "b9e919ab-5906-4fda-8a12-06e1feb115ff", "node_type": "1", "metadata": {"page_label": "12", "file_name": "paper.pdf", "file_path": "eigene/paper.pdf", "creation_date": "2023-11-09", "last_modified_date": "2023-10-20", "last_accessed_date": "2023-11-10"}, "hash": "3661d5f768029fb689743af0ea83cf2334cbf2a39fdfe8f9394ec84b248e12d3"}}, "hash": "72553fb40ef752784169a8d50cb3116b6953ba23bac474d1428a612901193aec", "text": "Turning towards  commercial design el ements , Tabassum et al.  (2019)  found tha t users would be \ncomfortable with an always listening function when sharing n on-sensitive or no n-perso nal \ninfor mation. It can be assume d that preferences and perceptions ab out voice de sign and individ ual \ndesign elements va ry per use case . Whil e the personif ication  of VAs through voice  design  cues and the  \ntailori ng towards users h as alread y been empirically inves tigated by a number o f studies, resea rch \nconsiderin g contextual g ranular ities, such as the di fferentiatio n betw een rela tional and transactional \ntasks  of VA deployment , is quite scarc e. More  so, the pr eviousl y mention ed studies show tha t voice \ncues alone do not affect of how a VA should be designed but ra ther in teract with the  context of the \nVA\u2019s deployment.  \n6 A Research Ag enda for Voice -Based Interactio n \nAs part of this section, we dis cuss th e contribu tions of  this stu dy\u2019s review  and provid e a research \nagenda. We see the import ance of progressing  the understanding of auditory cues for VAs  we arrived \nat as part of t his stud y by further exploring seemin gly contradictory findings as well  as exten ding our \ncategoris ation towards a more nuanced an d interlinked refinement of relevant VA design  cues.  \nElectronic copy available at: https://ssrn.com/abstract=3910609", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "84edadf0-4632-4891-b206-b2bc13b350d6": {"__data__": {"id_": "84edadf0-4632-4891-b206-b2bc13b350d6", "embedding": null, "metadata": {"page_label": "13", "file_name": "paper.pdf", "file_path": "eigene/paper.pdf", "creation_date": "2023-11-09", "last_modified_date": "2023-10-20", "last_accessed_date": "2023-11-10"}, "excluded_embed_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "fc365208-fe12-4f62-8f10-ed1b66d04c1b", "node_type": "4", "metadata": {"page_label": "13", "file_name": "paper.pdf", "file_path": "eigene/paper.pdf", "creation_date": "2023-11-09", "last_modified_date": "2023-10-20", "last_accessed_date": "2023-11-10"}, "hash": "4df274c5f1457e1338b358606ff772f67599dac2ab1a9b0e6748f4860d8b19e3"}, "3": {"node_id": "6261588e-302b-4a95-aa43-0ded02369dea", "node_type": "1", "metadata": {"page_label": "13", "file_name": "paper.pdf", "file_path": "eigene/paper.pdf", "creation_date": "2023-11-09", "last_modified_date": "2023-10-20", "last_accessed_date": "2023-11-10"}, "hash": "51dd347d226eb926e869b12381e211675c73e3b721e82e3d214d0fe8ebf7740a"}}, "hash": "eb5f604423f930828436a5d253fccfb31c10b92b2a76dce865c7724b815ecad5", "text": "Schmitt et al. / Voice Contemporary Frontier Int eraction De sign \nTwenty -Ninth European Conference on Information Systems (ECIS 2021), A Virtual AIS Conference.      12 Personific ation of VAs: Despite a great number of studies exploring the p ersonifi cation , or \nanthropomorp hising, of VAs, mixed res ults ha ve been foun d regarding  individu al dependent var iables, \nsuch as perc eptions of trust and naturalness.  These contr adicting findi ngs mi ght be due to differing  \nexpectations towards  a VAs across  contexts a nd tasks. Design strategi es in h uman -human interaction \nor pre vious human -computer i nteraction do not necessa rily ho ld for interact ions with VAs, especially \nin varying contexts and , hence, task requirements. For instance, non -humanlike spoken di alogue \ndesign h as proven successful f or redu cing spee ch collisio ns when interacting with a VA  (Funakoshi et \nal., 2010) . These res ults should be corroborated by fu rther studies and replicated i n different conte xts \nor with  different target groups  to under stand in wha t interacti ons personification is , for one, socially \nand ethically  desirable, and s econd, effective regarding the interaction go als from both user and \nprovider perspective. Building up on such empirica l testing , futu re res earch sho uld provide a more \nnuanced unde rstand ing of when to follow principles for human -like spoken dialo gue. \nIndividualization  of VA Inter action: Reviewed literature has demonstrated that a VA design matching  \nand consider ing individ ual user personality yield desira ble outcomes with regard to user per ception \nand interaction outcomes as comp ared to  a default, \u201cone s ize fit s all\u201d approach  and allow t o better \ncater the needs of previously n eglected target groups, such as elderly  (Torre  et al., 2018; Cambre and \nKulkarni, 2019). Besides privacy issue s (Dickhaut et al., 2021) , the matchi ng of use r personality, \nexpectations and prefe rences might bring  along (potentially) unforeseen  implications , which have  been \nneglected in research so far . For instance, t he matching of  gendered voice  to correspondin g gendere d \nfeatures and oc cupations p resents a double-edged sword  as highe r interactio n success  is accom plished \nwhile  reinforcing existing stereot ypes (Tay et al. , 2014) . Hence, we propose that resear chers  shoul d \nexplore design s trategies for VAs that combin e poten tially confli cting goals  (Dickhaut, E.; Janson, A. \n& Leimeister, 2020) . In other words , future studies sho uld investigate t he effect  of desig ning for the \nelimin ation or reduction of gender, occupationa l or geographica l stere otypes and re lated impact on \nsuccess and  percep tions of user interactio n (Tolmeijer  et al. , 2021) . The suggested res earch effo rts for \nboth pers onification and indi vidualization  could be tailored tow ards the specific  requir ements of \nencounters by comp aring needs and effectiveness of per sonification and indivi dualiz ation  between \ntransactional and relational tasks, for insta nce (Huang & Rust, 2020) .  \nContext Aw arenes s in VA Interaction: The conducted ana lysis and research agenda for the previous \ntwo design strategies have  emphasized  that the impact of voice is  very muc h context -dependent . The \ncontrib utions regarding  contextualization of user  intera ction with a VA are quite scarc e accordi ng to \nour understanding derive d from this study \u2019s review. Two different studies have show n that depending  \non task and c ontext , human-like turn taking , for instance,  might be more or le ss suitable .", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "6261588e-302b-4a95-aa43-0ded02369dea": {"__data__": {"id_": "6261588e-302b-4a95-aa43-0ded02369dea", "embedding": null, "metadata": {"page_label": "13", "file_name": "paper.pdf", "file_path": "eigene/paper.pdf", "creation_date": "2023-11-09", "last_modified_date": "2023-10-20", "last_accessed_date": "2023-11-10"}, "excluded_embed_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "fc365208-fe12-4f62-8f10-ed1b66d04c1b", "node_type": "4", "metadata": {"page_label": "13", "file_name": "paper.pdf", "file_path": "eigene/paper.pdf", "creation_date": "2023-11-09", "last_modified_date": "2023-10-20", "last_accessed_date": "2023-11-10"}, "hash": "4df274c5f1457e1338b358606ff772f67599dac2ab1a9b0e6748f4860d8b19e3"}, "2": {"node_id": "84edadf0-4632-4891-b206-b2bc13b350d6", "node_type": "1", "metadata": {"page_label": "13", "file_name": "paper.pdf", "file_path": "eigene/paper.pdf", "creation_date": "2023-11-09", "last_modified_date": "2023-10-20", "last_accessed_date": "2023-11-10"}, "hash": "eb5f604423f930828436a5d253fccfb31c10b92b2a76dce865c7724b815ecad5"}}, "hash": "51dd347d226eb926e869b12381e211675c73e3b721e82e3d214d0fe8ebf7740a", "text": "Furthermo re, \na number o f studies did  not furt her spec ify the u sage context and analysed  voice cues in simulated task  \nsettings and  isolated  lab env ironment s, which may imply a low external validit y of the current body of \nknowle dge that is also reflected i n the low num ber of co nducted f ield stu dies. A threat t o internal \nvalidity is posed by the study of multimodal  agents  which challenges  to isolate the eff ect o f voice-\nbased  interac tion in comparison to other modalities . The prevalence of st udies in task-separat ed, \nsimulated lab environmen ts lea ves questions around th e sui tability and effect of VAs deployed in \nencounters beyond the individual home una nswered. We propose conducting  field studies and \ncollecting \u201creal wor ld\u201d voice data when analys ing the implications of voice design for  interaction \ncontexts . \nMoreover, we have  identified some overarching research opportuniti es based on the reviewed \nliteratu re. We differentiated between auditory cues originating from the linguist ics literature and VA -\nspecific aud itory cues that arise with  the emergence of this novel system class. Thus, we see the future \ndevel opment of  two import ant streams  elaborati ng on th is differentiation : First, a holistic \nunderstanding of existing vocal cues in h uman -human interaction, relying upon the u nderstand ing and \nconceptual isations of other r esearch domains such as linguistics, should be de veloped tha t maps cues \nrelevant to human -VA interaction and hence demonstra tes the key differences between those two \ntypes o f interactions . Hall, Coats and LeBeau 's (2005)  categorisation of nonverbal cues offer s a \nstartin g point t o do so.  As part of this research effort, our cue categor isation shou ld be e xtended to  \nlinguistic  cue s to identify interrelati ons with speech -related cues  or even verbal cont ent other than \nElectronic copy available at: https://ssrn.com/abstract=3910609", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "8b1417e5-1925-457b-b055-10dd94cb9927": {"__data__": {"id_": "8b1417e5-1925-457b-b055-10dd94cb9927", "embedding": null, "metadata": {"page_label": "14", "file_name": "paper.pdf", "file_path": "eigene/paper.pdf", "creation_date": "2023-11-09", "last_modified_date": "2023-10-20", "last_accessed_date": "2023-11-10"}, "excluded_embed_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6f5d3c79-be03-473b-98bb-d11af3a1e2ef", "node_type": "4", "metadata": {"page_label": "14", "file_name": "paper.pdf", "file_path": "eigene/paper.pdf", "creation_date": "2023-11-09", "last_modified_date": "2023-10-20", "last_accessed_date": "2023-11-10"}, "hash": "92c0c038d44a8235201201de868eefbb99bf40a5717a3378a1ff0881e8d80569"}, "3": {"node_id": "b7744b70-a29c-4997-a953-4cdbd7f0cee1", "node_type": "1", "metadata": {"page_label": "14", "file_name": "paper.pdf", "file_path": "eigene/paper.pdf", "creation_date": "2023-11-09", "last_modified_date": "2023-10-20", "last_accessed_date": "2023-11-10"}, "hash": "3fcc0a80444ae59292c3adba336dc18648cfaf1766d88ffb548dfac0f5479605"}}, "hash": "fe1691389aecfb67d05ec0b43c41cd5e824935fb1297f825197731460f19aaea", "text": "Schmitt et al. / Voice Contemporary Frontier Int eraction De sign \nTwenty -Ninth European Conference on Information Systems (ECIS 2021), A Virtual AIS Conference.      13 auditory cues.  Second, vocal cues that arise through t he deployment  of synthetic voice  should be \nfurther inves tigated. With SSM L off ering a nov el convers ational speaking s tyle for VAs  (i.e., \nwhispering ), as well as a more granular level of personalization,  commercial products and actual \ndeployment might differ starkly  from our categori sation of voice -relevant cues . While some \npreliminary  research  on commer cially available  design guideli nes exist  (Branham & Roy, 2019) , we \nsee the devel opment of a comprehensive  overview  of existing and malleab le auditory cues in those  \ncommercially availa ble synthetic voice s as anot her important stream for future re search. As a f irst \nstep, a review of such commercially av ailable synthetic voice s and related librari es such as Google \nWavene t and Amazon Pol ly and also t he ecosystem  affordances should be conducted  (Knote et al., \n2021) . Consecutively, identified design cues  in commercially available VA s could be mapped aga inst \nthe curr ent overv iew o f features  identified  as part of this study.   \n7 Limita tions and Conclusion  \nWe co nsider several limi tation s of this study . First,  the scope of this literature review cannot be \nconsidered  fully exhaustive. However, to rea ch a high coverage, a dat abase -driven search stra tegy was \nchosen over an outlet -based  one to include  peer-reviewed conferenc e proceeding s and thus most \nnascent res earch on V As. To minimize the risk o f subjective bias, concise selection criteria  were \ndefined and followed up upon . Cover age may be re duced  due to the selection of the initial keyword \nsearch that was focused on di fferent terms u sed for VAs. Ho wever, we thoroughly analysed the \nsizeable amount of identified publications based on full -text analysis using a conce pt ma trix. As we  \nfocused  on vocal c ues whic h had already been empirically ex plored  in th e con text of (voice -based) \nconversa tional agen ts, we did not endeav our to provide a categori sation of auditory design cues that is \ncomplete in terms of an exhaustive  overview of  potentially relevant d esign cues. I n that se nse, our \ncategor isation is lacking a consideration of more dive rse l iteratu re from related domains, i.e. , \nlinguistics or psychology for a richer  understanding of what con stitutes voice. As ment ioned earlier, \nthe incl usion of more  diverse literature could  help to further contrast and progress the unde rstanding of \nhow voice in huma n-VA interacti on differs from human -human interaction. Future work may extend \nand substantiate our analy sis by including verbal con tent and style  (e.g., lexical emotional cont ent). \nThe inter -relatedness of the three proposed design strategi es shou ld also b e further addressed  in \nsubsequent research. While it is difficult t o discuss any of our findings detached fr om the contexts of \nthe unde rlying studies , all analyses and discuss ions should already be embedded within the notion of \ncontextualization .  \nConcludi ng, the holistic eval uation of th e progress of the specific research stream o n VAs and related \ndesign cu es is crucial to this stud y\u2019s consecutiv e ident ification of potential  research  fields and ga ps. As \na result , recomme ndations for f uture research o n the design and deployment  of VAs is p rovided. For \nthis purpos e, we conducted a system atic literature r eview to identify, code and analy se empirical \nfindings for VAs and related d esign features.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "b7744b70-a29c-4997-a953-4cdbd7f0cee1": {"__data__": {"id_": "b7744b70-a29c-4997-a953-4cdbd7f0cee1", "embedding": null, "metadata": {"page_label": "14", "file_name": "paper.pdf", "file_path": "eigene/paper.pdf", "creation_date": "2023-11-09", "last_modified_date": "2023-10-20", "last_accessed_date": "2023-11-10"}, "excluded_embed_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "6f5d3c79-be03-473b-98bb-d11af3a1e2ef", "node_type": "4", "metadata": {"page_label": "14", "file_name": "paper.pdf", "file_path": "eigene/paper.pdf", "creation_date": "2023-11-09", "last_modified_date": "2023-10-20", "last_accessed_date": "2023-11-10"}, "hash": "92c0c038d44a8235201201de868eefbb99bf40a5717a3378a1ff0881e8d80569"}, "2": {"node_id": "8b1417e5-1925-457b-b055-10dd94cb9927", "node_type": "1", "metadata": {"page_label": "14", "file_name": "paper.pdf", "file_path": "eigene/paper.pdf", "creation_date": "2023-11-09", "last_modified_date": "2023-10-20", "last_accessed_date": "2023-11-10"}, "hash": "fe1691389aecfb67d05ec0b43c41cd5e824935fb1297f825197731460f19aaea"}}, "hash": "3fcc0a80444ae59292c3adba336dc18648cfaf1766d88ffb548dfac0f5479605", "text": "By discu ssing the ke y insight s of our re view along three \ndesign s trategies , namely personi fication, individualization a nd co ntextualization, a set of research \nquestions for future research in the cont ext of VAs was devel oped.  \n \n \n \n \n \nAcknowledgements  \nWe thank t he Swiss National Science Foundation fo r funding parts of this research (100013_192718).  \nThe third author also acknowledg es funding from the Ba sic Re search Fund ( GFF) of the University of  \nSt. Gallen .  \n \n \nElectronic copy available at: https://ssrn.com/abstract=3910609", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "8eb97403-a869-4bdf-a5d8-31f7260f557a": {"__data__": {"id_": "8eb97403-a869-4bdf-a5d8-31f7260f557a", "embedding": null, "metadata": {"page_label": "15", "file_name": "paper.pdf", "file_path": "eigene/paper.pdf", "creation_date": "2023-11-09", "last_modified_date": "2023-10-20", "last_accessed_date": "2023-11-10"}, "excluded_embed_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "61670ee1-a3d2-4abb-967a-11acf5e7462e", "node_type": "4", "metadata": {"page_label": "15", "file_name": "paper.pdf", "file_path": "eigene/paper.pdf", "creation_date": "2023-11-09", "last_modified_date": "2023-10-20", "last_accessed_date": "2023-11-10"}, "hash": "be0b299a7dba161307b25db69e9688b2c2baeaf4f15c22a3ede62b0a8f5e0449"}, "3": {"node_id": "d859464f-8e96-4d9d-8e25-d5d76e4c4ec9", "node_type": "1", "metadata": {"page_label": "15", "file_name": "paper.pdf", "file_path": "eigene/paper.pdf", "creation_date": "2023-11-09", "last_modified_date": "2023-10-20", "last_accessed_date": "2023-11-10"}, "hash": "d807f55e793ed0159ff9ef9e65e6c18c60e9ce08e3d0420abf8ee5ce5162758a"}}, "hash": "10952c66aa57dcf651bd852fe19c182ed7ed467b16b0be83c599c4dafece23d5", "text": "Schmitt et al. / Voice Contemporary Frontier Int eraction De sign \nTwenty -Ninth European Conference on Information Systems (ECIS 2021), A Virtual AIS Conference.      14  \nReferences  \n \nAbdolrahmani, A., Kuber, R., & Branham, S. M. (2018). Si ri talks at you: An empirical investigation \nof voice -activated personal assistant (VAPA) usage by individuals who are blind. ASSETS 2018 - \nProceedings of the 20th International ACM SIGACCESS Conference on Computers and \nAccessibility , 249 \u2013258. https://doi.o rg/10.1145/3234695.3236344  \nAeschlimann, S., Bleiker, M., Wechner, M., & Gampe, A. (2020). Communicative and social \nconsequenc es of interactions with voice assistants. Computers in Human Behavior , 112, 106466. \nhttps://doi.org/10.1016/j.chb.2020.106466  \nBalas uriya, S. S., Sitbon, L., Bayor, A. A., Hoogstrate, M., & Brereton, M. (2018). Use of voice \nactivated interfaces by people wi th intellectual disability. ACM International Conference \nProceeding Series , 102 \u2013112. https://doi.org/10.1145/3292147.3292161  \nBelin,  P., Fecteau, S., & B \u00e9dard, C. (2004). Thinking the voice: Neural correlates of voice perception. \nTrends in Cognitive Science s, 8(3), 129 \u2013135. https://doi.org/10.1016/j.tics.2004.01.008  \nBerry, D. C., Butler, L. T., & De Rosis, F. (2005). Evaluating a reali stic agent in an advice -giving task. \nInternational Journal of Human Computer Studies , 63(3), 304 \u2013327. \nhttps://doi.org/10.1016 /j.ijhcs.2005.03.006  \nBranham, S. M., & Roy, A. R. M. (2019). Reading between the guidelines: How commercial voice \nassistant guideli nes hinder accessibility for blind users. ASSETS 2019 - 21st International ACM \nSIGACCESS Conference on Computers and Accessib ility, 446 \u2013458. \nhttps://doi.org/10.1145/3308561.3353797  \nBraun, M., Mainz, A., Chadowitz, R., Pfleging, B., & Alt, F. (2019). At you r service: Designing voice \nassistant personalities to improve automotive user interfaces a real world driving study. \nConferen ce on Human Factors in Computing Systems - Proceedings , 1 \u201311. \nhttps://doi.org/10.1145/3290605.3300270  \nCambre, J., & Kulkarni, C. (2 019). One voice fits all? Social implications and research challenges of \ndesigning voices for smart devices. Proceedings of t he ACM on Human -Computer Interaction , \n3(CSCW). https://doi.org/10.1145/3359325  \nCarlotto, T., & Jaques, P. A. (2016). The effects of  animated pedagogical agents in an English -as-a-\nforeign -language learning environment. International Journal of Human Compute r Studies , 95, \n15\u201326. https://doi.org/10.1016/j.ijhcs.2016.06.001  \nCarpenter, J., Davis, J. M., Erwin -Stewart, N., Lee, T. R., Brans ford, J. D., & Vye, N. (2009). Gender \nrepresentation and humanoid robots designed for domestic use. International Journal of Social \nRobotics , 1(3), 261 \u2013265. https://doi.org/10.1007/s12369 -009-0016 -4 \nChang, R. C. S., Lu, H. P., & Yang, P. (2018).", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "d859464f-8e96-4d9d-8e25-d5d76e4c4ec9": {"__data__": {"id_": "d859464f-8e96-4d9d-8e25-d5d76e4c4ec9", "embedding": null, "metadata": {"page_label": "15", "file_name": "paper.pdf", "file_path": "eigene/paper.pdf", "creation_date": "2023-11-09", "last_modified_date": "2023-10-20", "last_accessed_date": "2023-11-10"}, "excluded_embed_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "61670ee1-a3d2-4abb-967a-11acf5e7462e", "node_type": "4", "metadata": {"page_label": "15", "file_name": "paper.pdf", "file_path": "eigene/paper.pdf", "creation_date": "2023-11-09", "last_modified_date": "2023-10-20", "last_accessed_date": "2023-11-10"}, "hash": "be0b299a7dba161307b25db69e9688b2c2baeaf4f15c22a3ede62b0a8f5e0449"}, "2": {"node_id": "8eb97403-a869-4bdf-a5d8-31f7260f557a", "node_type": "1", "metadata": {"page_label": "15", "file_name": "paper.pdf", "file_path": "eigene/paper.pdf", "creation_date": "2023-11-09", "last_modified_date": "2023-10-20", "last_accessed_date": "2023-11-10"}, "hash": "10952c66aa57dcf651bd852fe19c182ed7ed467b16b0be83c599c4dafece23d5"}}, "hash": "d807f55e793ed0159ff9ef9e65e6c18c60e9ce08e3d0420abf8ee5ce5162758a", "text": "(2018). Stereotype s or golden rules? Exploring likable voice \ntraits of social robots as active aging companions for tech -savvy baby boomers in Taiwan. \nComputers in Human Behavior , 84, 194 \u2013210. https://doi.org/10.1016/j.chb.2018.02.025  \nChidambaram, V., Chiang, Y. H., & Mutlu , B. (2012). Designing persuasive robots: How robots might \npersuade people using vocal and nonverbal cues. HRI\u201912 - Proceedin gs of the 7th Annual \nACM/IEEE International Conference on Human -Robot Interaction , 293 \u2013300. \nhttps://doi.org/10.1145/2157689.2157798  \nCho, E., Molina, M. D., & Wang, J. (2019). The Effects of Modality, Device, and Task Differences on \nPerceived Human Likeness  of Voice -Activated Virtual Assistants. Cyberpsychology, Behavior, \nand Social Networking , 22(8), 515 \u2013520. https://doi.org/10.1089/c yber.2018.0571  \nCowan, B. R., Gannon, D., Walsh, J., Kinneen, J., O \u2019keefe, E., & Xie, L. (2016). Towards \nunderstanding how spe ech output affects navigation system credibility. Conference on Human \nFactors in Computing Systems - Proceedings , 07-12-May-, 2805 \u20132812. \nhttps://doi.org/10.1145/2851581.2892469  \nCraig, S. D., & Schroeder, N. L. (2017). Reconsidering the voice effect when le arning from a virtual \nElectronic copy available at: https://ssrn.com/abstract=3910609", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "b0531301-e088-4b8b-a2da-22c0b1397343": {"__data__": {"id_": "b0531301-e088-4b8b-a2da-22c0b1397343", "embedding": null, "metadata": {"page_label": "16", "file_name": "paper.pdf", "file_path": "eigene/paper.pdf", "creation_date": "2023-11-09", "last_modified_date": "2023-10-20", "last_accessed_date": "2023-11-10"}, "excluded_embed_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e7b68b7d-f8b6-4cbb-884b-ac86a27a6728", "node_type": "4", "metadata": {"page_label": "16", "file_name": "paper.pdf", "file_path": "eigene/paper.pdf", "creation_date": "2023-11-09", "last_modified_date": "2023-10-20", "last_accessed_date": "2023-11-10"}, "hash": "a9a985a30e1614acddcf7c81561396f45775ed4304cbde3a94ef5ac702c7be45"}, "3": {"node_id": "ac6c9336-7018-445c-9555-cae5cf448e22", "node_type": "1", "metadata": {"page_label": "16", "file_name": "paper.pdf", "file_path": "eigene/paper.pdf", "creation_date": "2023-11-09", "last_modified_date": "2023-10-20", "last_accessed_date": "2023-11-10"}, "hash": "03d2102f2640892b8d9c663d4bf942c9aea9b830092c3650bcea7dc7a6011383"}}, "hash": "4d645c987e1a41955fc88ef8718c045171ab2a9ad50d6e7b8e303141388efaa4", "text": "Schmitt et al. / Voice Contemporary Frontier Int eraction De sign \nTwenty -Ninth European Conference on Information Systems (ECIS 2021), A Virtual AIS Conference.      15 human. Computers and Education , 114, 193 \u2013205. \nhttps://doi.org/10.1016/j.compedu.2017.07.003  \nDennis, A. R., Fu ller, R. M., & Valacich, J. S. (2008). Media, Tasks, and Communication Processes: \nA Theory of Media Synchronicity. MIS Quarte rly, 32(3), 575 \u2013600. \nDickhaut, E.; Janson, A. & Leimeister, J. M. (2020). Codifying Interdisciplinary Design Knowledge \nthrough Patt erns \u2013 The Case of Smart Personal Assistants. In Design Science Research in \nInformation Systems and Technology (DESRIST)  (pp. 114\u2013125). Springer International \nPublishing.  \nDickhaut, E., Li, M., Janson, A., & Leimeister, J. M. (2021). Developing Lawful Techn ologies \u2013 A \nRevelatory Case Study on Design Patterns. Proceedings of the 54th Hawaii International \nConference on System Scien ces, 54. https://doi.org/10.24251/hicss.2021.533  \nElkins, A. C., & Derrick, D. C. (2013). The Sound of Trust: Voice as a Measurement  of Trust During \nInteractions with Embodied Conversational Agents. Group Decision and Negotiation , 22(5), \n897\u2013913. https://do i.org/10.1007/s10726 -012-9339 -x \nEyssel, F., Kuchenbrandt, D., Bobinger, S., De Ruiter, L., & Hegel, F. (2012). \u201cIf you sound like m e, \nyou must be more human \u201d: On the interplay of robot and user features on human -robot \nacceptance and anthropomorphism. HRI\u201912 - Proceedings of the 7th Annual ACM/IEEE \nInternational Conference on Human -Robot Interaction , 125 \u2013126. \nhttps://doi.org/10.1145/21 57689.2157717  \nFeine, J., Gnewuch, U., Morana, S., & Maedche, A. (2019). A Taxonomy of Social Cues for \nConversational Agents. International Journal of Human Computer Studies , 132(June), 138 \u2013161. \nhttps://doi.org/10.1016/j.ijhcs.2019.07.009  \nFischer Stuart Ree ves Martin Porcheron, J. E. (2019). Progressivity for Voice Interface Design Rein \nOve Sikveland . ii. https://doi.org/10.1145/ 3342775.  \nFunakoshi, K., Nakano, M., Kobayashi, K., Komatsu, T., & Yamada, S. (2010). Non -humanlike \nspoken dialogue: A design perspe ctive. Proceedings of the SIGDIAL 2010 Conference: 11th \nAnnual Meeting of the Special Interest Group OnDiscourse and Dialogue , 176 \u2013184. \nGravano, A., & Hirschberg, J. (2011). Turn -taking cues in task -oriented dialogue. Computer Speech \nand Language , 25(3), 6 01\u2013634. https://doi.org/10.1016/j.csl.2010.10.003  \nGriffin, E., Ledbetter, A., & Sparks, G. G. (2018). A First Look at Communi cation Theory  (10th ed.). \nMcGraw -Hill Education.  \nHall, J. A., Coats, E. J., & LeBeau, L. S. (2005). Nonverbal behavior and the vert ical dimension of \nsocial relations: A meta -analysis. Psychological Bulletin , 131(6), 898 \u2013924.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "ac6c9336-7018-445c-9555-cae5cf448e22": {"__data__": {"id_": "ac6c9336-7018-445c-9555-cae5cf448e22", "embedding": null, "metadata": {"page_label": "16", "file_name": "paper.pdf", "file_path": "eigene/paper.pdf", "creation_date": "2023-11-09", "last_modified_date": "2023-10-20", "last_accessed_date": "2023-11-10"}, "excluded_embed_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "e7b68b7d-f8b6-4cbb-884b-ac86a27a6728", "node_type": "4", "metadata": {"page_label": "16", "file_name": "paper.pdf", "file_path": "eigene/paper.pdf", "creation_date": "2023-11-09", "last_modified_date": "2023-10-20", "last_accessed_date": "2023-11-10"}, "hash": "a9a985a30e1614acddcf7c81561396f45775ed4304cbde3a94ef5ac702c7be45"}, "2": {"node_id": "b0531301-e088-4b8b-a2da-22c0b1397343", "node_type": "1", "metadata": {"page_label": "16", "file_name": "paper.pdf", "file_path": "eigene/paper.pdf", "creation_date": "2023-11-09", "last_modified_date": "2023-10-20", "last_accessed_date": "2023-11-10"}, "hash": "4d645c987e1a41955fc88ef8718c045171ab2a9ad50d6e7b8e303141388efaa4"}}, "hash": "03d2102f2640892b8d9c663d4bf942c9aea9b830092c3650bcea7dc7a6011383", "text": "Psychological Bulletin , 131(6), 898 \u2013924. \nhttps://doi.org/10.1037/0033 -2909.131.6.898  \nHerm -stapelberg, N., Mainz, J. G., H erm-stapelberg, N., & Mainz, J. G. (2020). AIS Electronic \nLibrary ( AISeL ) The Im pact of Gender Stereotyping on the Perceived Likability of Virtual \nAssistants The Impact of Gender Stereotyping on the Perceived Likability of Virtual Assistants . \n0\u20137. \nHess, T., Fuller, M., & Campbell, D. (2009). Journal of the Association for Information Systems \nDesigning Interfaces with Social Presence \u202f: Using Vividness and Extraversion to Create Social \nRecommendation Agents * Designing Interfaces with Social Presence \u202f: Using Vividness and \nExtraversion to Create. Journal of the Association for Information  Systems , 10(12), 889 \u2013919. \nHildebrand, C., & Bergner, A. (2020). Conversational Robo Advisors as Surrogates of Trust: \nOnboarding Experience, Firm Perception, and Consum er Financial Decision Making. Journal of \nthe Academy of Marketing Science, Forthcoming . \nHuang, M. H., & Rust, R. T. (2020). Engaged to a Robot? The Role of AI in Service. Journal of \nService Research . https://doi.org/10.1177/1094670520902266  \nHwang, G., Oh, C. Y., Lee, J., & Lee, J. (2019). It sounds like a woman: Exploring gender stereotypes \nin South Korean voice assistants. Conference on Human Factors in Computing Systems - \nProceedings , 1\u20136. https://doi.org/10.1145/3290607.3312915  \nKhaled, F. (2021, April 1) . Apple removes Siri \u2019s female voice as its default and adds two new voices. \nBusiness Ins ider. https://www.businessinsider.com/apple -removes -siri-female -voice -as-its-\ndefault -2021 -4?r=US&IR=T#:~:text=Siri currently defaults to a,African%2C Irish%2C and \nElectronic copy available at: https://ssrn.com/abstract=3910609", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "bd4c514c-6885-4f11-9378-ef1c4d37927f": {"__data__": {"id_": "bd4c514c-6885-4f11-9378-ef1c4d37927f", "embedding": null, "metadata": {"page_label": "17", "file_name": "paper.pdf", "file_path": "eigene/paper.pdf", "creation_date": "2023-11-09", "last_modified_date": "2023-10-20", "last_accessed_date": "2023-11-10"}, "excluded_embed_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2d51d47a-d405-43bc-a8ca-9de1732f8548", "node_type": "4", "metadata": {"page_label": "17", "file_name": "paper.pdf", "file_path": "eigene/paper.pdf", "creation_date": "2023-11-09", "last_modified_date": "2023-10-20", "last_accessed_date": "2023-11-10"}, "hash": "98eb92e2c2ae2a589f8ba11cc1eb3545a577c2c61b0a8cfdc49c9c7601f22d54"}, "3": {"node_id": "ca794b48-97b4-438b-b9cc-2af186639bcc", "node_type": "1", "metadata": {"page_label": "17", "file_name": "paper.pdf", "file_path": "eigene/paper.pdf", "creation_date": "2023-11-09", "last_modified_date": "2023-10-20", "last_accessed_date": "2023-11-10"}, "hash": "ad94f8d6650d51094608d5386642634acb852158cc4335e7c68cc096c59944e5"}}, "hash": "acc3f3e053d3238e943c3b22a42552fc75da70b31e9d1417abfba8ada6c63e31", "text": "Schmitt et al. / Voice Contemporary Frontier Int eraction De sign \nTwenty -Ninth European Conference on Information Systems (ECIS 2021), A Virtual AIS Conference.      16 Austr alian.  \nKnote, R., Janson, A., S \u00f6llner, M., & Leimeister, J. M. (2019). Classifying Smart  Personal Assistants: \nAn Empirical Cluster Analysis. Proceedings of the 52nd Hawaii International Conference on \nSystem Sciences , 6, 2024 \u20132033. https://doi.org/10.24251/ hicss.2019.245  \nKnote, R., Janson, A., S \u00f6llner, M., & Leimeister, J. M. (2021). Value co -creation in smart services: A \nfunctional affordances perspective on smart personal assistants. Journal of the Association for \nInformation Systems , 22(2), 418 \u2013458. https ://doi.org/10.17705/1jais.00667  \nKomiak, S. X., & Benbasat, I. (2003). Understanding Cust omer Trust in Agent -Mediated Electronic \nCommerce, Web -Mediated Electronic Commerce, and Traditional Commerce. Information \nTechnology and Management , 5(1/2), 181 \u2013207. \nhttps://doi.org/10.1023/b:item.0000008081.55563.d4  \nLow, C. (2020, September). Alexa will s eem more human with breathing pauses and learning skills. \nEndgadget . https://www.engadget.com/amazon -2020 -alexa -breathing -teach -voice -profiles -for-\nkids-172918631.html  \nLubold, N., Walker, E., & Pon -Barry, H. (2020). Effects of adapting to user pitch on rapp ort \nperception, behavior, and state with a social robotic learning companion. In User Modeling and \nUser -Adapted Interaction  (Issue 0123456789). Springer Netherlands. \nhttps://doi.org/10.1007/s11257 -020-09267 -3 \nLusk, M. M., & Atkinson, R. K. (2007). Animated  Pedagogical Agents: Does Their Degree of \nEmbodiement Impact Learning from Static or Animated Worked Examples? Applied Cognitive \nPsychology , 21, 747 \u2013764. \nMania, J., Mie dema, F., Browne, R., Broekens, J., & Oertel, C. (2020). Towards Understanding the \nEffec t of Voice on Human -Agent Negotiation . 1\u20138. https://doi.org/10.1145/3383652.3423896  \nMarge, M., Miranda, J., Black, A. W., & Rudnicky, A. I. (2010). Towards improving th e naturalness \nof social conversations with dialogue systems. Proceedings of the SIGDIAL 2010 Conference: \n11th Annual Meeting of the Special Interest Group OnDiscourse and Dialogue , 91\u201394. \nMasina, F., Orso, V., Pluchino, P., Dainese, G., Volpato, S., Nelini , C., Mapelli, D., Spagnolli, A., & \nGamberini, L. (2020). Investigating the accessibilit y of voice assistants with impaired users: \nMixed methods study. Journal of Medical Internet Research , 22(9). \nhttps://doi.org/10.2196/18431  \nMcTear, M. F. (2017). The Ris e of the Conversational Interface: A New Kid on the Block? In Future \nand Emerging Trends  in Language Technology  (pp. 38 \u201349). Springer International Publishing.  \nMurad, C., Munteanu, C., Cowan, B. R., & Clark, L. (2019). Revolution or Evolution? Speech \nInteraction and HCI Design Guidelines. IEEE Pervasive Computing , 18(2), 33 \u201345.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "ca794b48-97b4-438b-b9cc-2af186639bcc": {"__data__": {"id_": "ca794b48-97b4-438b-b9cc-2af186639bcc", "embedding": null, "metadata": {"page_label": "17", "file_name": "paper.pdf", "file_path": "eigene/paper.pdf", "creation_date": "2023-11-09", "last_modified_date": "2023-10-20", "last_accessed_date": "2023-11-10"}, "excluded_embed_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "2d51d47a-d405-43bc-a8ca-9de1732f8548", "node_type": "4", "metadata": {"page_label": "17", "file_name": "paper.pdf", "file_path": "eigene/paper.pdf", "creation_date": "2023-11-09", "last_modified_date": "2023-10-20", "last_accessed_date": "2023-11-10"}, "hash": "98eb92e2c2ae2a589f8ba11cc1eb3545a577c2c61b0a8cfdc49c9c7601f22d54"}, "2": {"node_id": "bd4c514c-6885-4f11-9378-ef1c4d37927f", "node_type": "1", "metadata": {"page_label": "17", "file_name": "paper.pdf", "file_path": "eigene/paper.pdf", "creation_date": "2023-11-09", "last_modified_date": "2023-10-20", "last_accessed_date": "2023-11-10"}, "hash": "acc3f3e053d3238e943c3b22a42552fc75da70b31e9d1417abfba8ada6c63e31"}}, "hash": "ad94f8d6650d51094608d5386642634acb852158cc4335e7c68cc096c59944e5", "text": "IEEE Pervasive Computing , 18(2), 33 \u201345. \nhttps://doi.or g/10.1109/MPRV.2019.2906991  \nNass, C., & Lee, K. M. (2001). Does computer -synthesized speech manifest personality? Experimental \ntests of recognition, similarity -attracti on, and consistency -attraction. Journal of Experimental \nPsychology: Applied , 7(3), 171 \u2013181. https://doi.org/10.1037/1076 -898X.7.3.171  \nNguyen, Q. N., Ta, A., & Prybutok, V. (2019). An Integrated Model of Voice -User Interface \nContinuance Intention: The Gende r Effect. International Journal of Human -Computer \nInteraction , 35(15), 1362 \u20131377. https: //doi.org/10.1080/10447318.2018.1525023  \nO\u2019Brien, K., Liggett, A., Ramirez -Zohfeld, V., Sunkara, P., & Lindquist, L. A. (2020). Voice -\nControlled Intelligent Personal Ass istants to Support Aging in Place. Journal of the American \nGeriatrics Society , 68(1), 17 6\u2013179. https://doi.org/10.1111/jgs.16217  \nObinali, C. (2019). The Perception of Gender in Voice Assistants. Proceedings of the 22nd Southern \nAssociation for Information Systems Conference (SAIS) , 1\u20136. https://aisel.aisnet.org/sais2019/39  \nPagani, M., Racat, M., & Hofacker, C. F. (2019). Adding Voice to the Omnichannel and How that \nAffects Brand Trust. Journal of Interactive Marketing , 48, 89 \u2013105. \nhttps://doi.org/10.1016/j. intmar.2019.05.002  \nPfeuffer, N., Benlian, A., Gimpel, H., & Hinz, O. (2019). Anthropomor phic Information Systems. \nBusiness & Information Systems Engineering , 61(4), 523 \u2013533. https://doi.org/10.1007/s12599 -\n019-00599 -y \nQiu, L., & Benbasat, I. (2005). Online consumer trust and live help interfaces: The effects of text -to-\nElectronic copy available at: https://ssrn.com/abstract=3910609", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "3264478c-66c0-4c4a-b218-35512f5a2179": {"__data__": {"id_": "3264478c-66c0-4c4a-b218-35512f5a2179", "embedding": null, "metadata": {"page_label": "18", "file_name": "paper.pdf", "file_path": "eigene/paper.pdf", "creation_date": "2023-11-09", "last_modified_date": "2023-10-20", "last_accessed_date": "2023-11-10"}, "excluded_embed_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8ecc7d75-5871-4171-a74c-64872af2d05e", "node_type": "4", "metadata": {"page_label": "18", "file_name": "paper.pdf", "file_path": "eigene/paper.pdf", "creation_date": "2023-11-09", "last_modified_date": "2023-10-20", "last_accessed_date": "2023-11-10"}, "hash": "aed0f667c42251c3f99f49720d0b4f761b9e93c75d30fcdaa00aac4e08112ea3"}, "3": {"node_id": "85948b7c-d794-41f6-a9d7-bf083d155548", "node_type": "1", "metadata": {"page_label": "18", "file_name": "paper.pdf", "file_path": "eigene/paper.pdf", "creation_date": "2023-11-09", "last_modified_date": "2023-10-20", "last_accessed_date": "2023-11-10"}, "hash": "902eaaeda7a52e486e379d635c6a1f6863df6c9176f3b81e7422a8ab64c8ce75"}}, "hash": "a5bca08c4f41400b4d98c26d6b9cbcfb0c02066195be75921577983c67a10eac", "text": "Schmitt et al. / Voice Contemporary Frontier Int eraction De sign \nTwenty -Ninth European Conference on Information Systems (ECIS 2021), A Virtual AIS Conference.      17 speech voice and three -dimensional avatars. International Journal of Human -Computer \nInteraction , 19(1), 75 \u201394. https://doi.org/10.1207/s15327590ijhc1901_6  \nRedeker, G. (1984). On Differences Be tween Spoken and Written Language. Discourse Processes , \n7(1), 43 \u201355. https://doi.org/10. 1080/01638538409544580  \nReeves, B., & Nass, C. I. (1996). The media equation: How people treat computers; television; and \nnew media like real people and places.  Center f or the Study of Language and Information; \nCambridge University Press.  \nRosenthal, G. G., & Ryan, M. J. (2000). Visual and Acosutic Communication in Non -Human \nAnimals: A Comparison. Indian Academy of Sciences , 25(3), 285 \u2013290. \nRubin, D. L., Hafer, T., & Arata, K. (2000). Reading and listening to oral -based versus literate -based  \ndiscourse. Commun ication Education , 49(2), 121 \u2013133. \nhttps://doi.org/10.1080/03634520009379200  \nSchl\u00f6gl, S., Chollet, G., Garschall, M., Tscheligi, M., & Legouverneur, G. (2013). Exploring voice \nuser interfaces for seniors. ACM International Conference Proc eeding Series , 2\u20133. \nhttps://doi.org/10.1145/2504335.2504391  \nSchuetzler, R. M., Grimes, G. M., & Rosser, H. K. (2021). Deciding Whether and How to Deploy \nChatbots Deciding Whether and How to Deploy Chatbots . 20(1). \nhttps://doi.org/10.17705/2msqe.00039  \nShi, Y., Lou, Y., Yan, X., Cao, N., & Ma, X. (2018). Designing emotional expressions of \nconversational states for voice assistants: Modality and engagement. Conference on Human \nFactors in Computing Systems - Procee dings , 2018 -April , 1 \u20136. \nhttps://doi.org/10.1145/3170427.3188560  \nSmith, S. (2020, April). Juniper Research: Number of Voice Assistant Devices in Use to Overtake \nWorld Population by 2024, Reaching 8.4bn, Led by Smartphones. Businesswire . \nhttps://www.business wire.com/news/home/20200427005609/en/Juniper -Research -Number -\nVoic e-Assistant -Devices -Overtake  \nStra\u00dfmann, C., Kr \u00e4mer, N. C., Buschmeier, H., & Kopp, S. (2020). Age -Related Differences in the \nEvaluation of a Virtual Health Agent \u2019s Appearance and Embodiment i n a Health -Related \nInteraction: Experimental Lab Study. Journal o f Medical Internet Research , 22(4), 1 \u201315. \nhttps://doi.org/10.2196/13726  \nSutton, S. J. (2020). Gender Ambiguous, not Genderless: Designing Gender in Voice User Interfaces \n(VUIs) with Sensitivi ty. ACM International Conference Proceeding Series , July 2017 . \nhttps://doi.org/10.1145/3405755.3406123  \nSutton, S. J., Foulkes, P., Kirk, D., & Lawson, S. (2019). Voice as a design material: Sociophonetic \ninspired design strategies in human -computer interac tion. Conference on Human Factors in \nComputing Systems - Proceedi ngs, 1\u201314.", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "85948b7c-d794-41f6-a9d7-bf083d155548": {"__data__": {"id_": "85948b7c-d794-41f6-a9d7-bf083d155548", "embedding": null, "metadata": {"page_label": "18", "file_name": "paper.pdf", "file_path": "eigene/paper.pdf", "creation_date": "2023-11-09", "last_modified_date": "2023-10-20", "last_accessed_date": "2023-11-10"}, "excluded_embed_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "8ecc7d75-5871-4171-a74c-64872af2d05e", "node_type": "4", "metadata": {"page_label": "18", "file_name": "paper.pdf", "file_path": "eigene/paper.pdf", "creation_date": "2023-11-09", "last_modified_date": "2023-10-20", "last_accessed_date": "2023-11-10"}, "hash": "aed0f667c42251c3f99f49720d0b4f761b9e93c75d30fcdaa00aac4e08112ea3"}, "2": {"node_id": "3264478c-66c0-4c4a-b218-35512f5a2179", "node_type": "1", "metadata": {"page_label": "18", "file_name": "paper.pdf", "file_path": "eigene/paper.pdf", "creation_date": "2023-11-09", "last_modified_date": "2023-10-20", "last_accessed_date": "2023-11-10"}, "hash": "a5bca08c4f41400b4d98c26d6b9cbcfb0c02066195be75921577983c67a10eac"}}, "hash": "902eaaeda7a52e486e379d635c6a1f6863df6c9176f3b81e7422a8ab64c8ce75", "text": "https://doi.org/10.1145/3290605.3300833  \nTabassum, M., Kosinski, T., Frik, A., Malkin, N., Wijesekera, P., Egelman, S., & Lipford, H. R. \n(2019). Investigating users \u2019 preferences and  expectations for always -listening voice assistants. \nProceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies , 3(4). \nhttps://doi.org/10.1145/3369807  \nTamagawa, R., Watson, C. I., Kuo, I. H., Macdonald, B. A., & Broadbent, E. (2011 ). The effects of \nsynthesized voice accents on user perceptions o f robots. International Journal of Social Robotics , \n3(3), 253 \u2013262. https://doi.org/10.1007/s12369 -011-0100 -4 \nTarantola, A. (2020, November). Alexa is getting better at guessing your intention s. Endgadget . \nhttps://www.engadget.com/alexa -is-getting -better -at-guessing -your-intentions -202813759.html  \nTay, B., Jung, Y., & Park, T. (2014). When stereotypes meet robots: The double -edge sword of robot \ngender and personality in human -robot interaction. Computers in Human Behavior , 38, 75\u201384. \nhttps://doi.org/10.1016/j .chb.2014.05.014  \nTolmeijer, S.; Zierau, N.; Janson, A.; Wahdatehagh, J. S.; Leimeister; J. M. & Bernstein, A. (2021). \nFemale by Default? \u2013 Exploring the Effect of Voice Assistant Gender and P itch on Trait and \nTrust Attribution. ACM CHI 2021 Late Breaking W ork. \nTorre, I., Goslin, J., & White, L. (2020). If your device could smile: People trust happy -sounding \nartificial agents more. Computers in Human Behavior , 105(December 2019), 106215. \nElectronic copy available at: https://ssrn.com/abstract=3910609", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "3327329d-4bdc-4787-901d-047cb1eb6bbe": {"__data__": {"id_": "3327329d-4bdc-4787-901d-047cb1eb6bbe", "embedding": null, "metadata": {"page_label": "19", "file_name": "paper.pdf", "file_path": "eigene/paper.pdf", "creation_date": "2023-11-09", "last_modified_date": "2023-10-20", "last_accessed_date": "2023-11-10"}, "excluded_embed_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "517c3417-3654-4b8a-bcbd-e5f226c1cff1", "node_type": "4", "metadata": {"page_label": "19", "file_name": "paper.pdf", "file_path": "eigene/paper.pdf", "creation_date": "2023-11-09", "last_modified_date": "2023-10-20", "last_accessed_date": "2023-11-10"}, "hash": "0414edfb8bcfcaafc2a5206b8488cc0f07ec694fa622f57d14532ff03f303a77"}, "3": {"node_id": "723cfa19-9362-42a9-b04f-f1688916185d", "node_type": "1", "metadata": {"page_label": "19", "file_name": "paper.pdf", "file_path": "eigene/paper.pdf", "creation_date": "2023-11-09", "last_modified_date": "2023-10-20", "last_accessed_date": "2023-11-10"}, "hash": "9d9269722404db85cfa5b64d83213e28e5a1b8a59051a3442ccf0f4642ab7522"}}, "hash": "3e25c0b2fabd045f2555e671d2ba06f630f81dcdc96cbfe82795bab763eec7f2", "text": "Schmitt et al. / Voice Contemporary Frontier Int eraction De sign \nTwenty -Ninth European Conference on Information Systems (ECIS 2021), A Virtual AIS Conference.      18 https: //doi.org/10.1016/j.chb.2019.106215  \nTorre, I., Goslin, J., White,  L., & Zanatto, D. (2018). Trust in artificial voices: A \u201ccongruency effect \u201d \nof first impressions and behavioural experience. ACM International Conference Proceeding \nSeries , April . https://do i.org/10.1145/3183654.3183691  \nTorre, I., Latupeirissa, A. B., & M cGinn, C. (2020). How context shapes the appropriateness of a \nrobot\u2019s voice . 215 \u2013222. https://doi.org/10.1109/ro -man47096.2020.9223449  \nTrovato, G., Lopez, A., Paredes, R., & Cuellar, F. (2017 ). Security and guidance: Two roles for a \nhumanoid robot in an in teraction experiment. RO-MAN 2017 - 26th IEEE International \nSymposium on Robot and Human Interactive Communication , 2017 -Janua , 230 \u2013235. \nhttps://doi.org/10.1109/ROMAN.2017.8172307  \nTruschin, S ., Schermann, M., Goswami, S., & Krcmar, H. (2014). Designing int erfaces for multiple -\ngoal environments: Experimental insights from in -vehicle speech interfaces. ACM Transactions \non Computer -Human Interaction , 21(1). https://doi.org/10.1145/2544066  \nV\u00f6lkel, S. T., Sch \u00f6del, R., Buschek, D., Stachl, C., Winterhalter, V., B \u00fchner, M., & Hussmann, H. \n(2020). Developing a Personality Model for Speech -based Conversational Agents Using the \nPsycholexical Approach. Conference on Human Factors in Computing Systems - Proceedings . \nhttps://doi.org/10.1145/3313831.3376210  \nWebster, J., &  Watson, R. T. (2002). Analyzing the Past to Prepare for the Future: Writing a Literature \nReview. MIS Quarterly , 26(2), xiii \u2013xxiii. https://doi.org/10.1.1.104.6570  \nWellnhammer, N., Dolata, M. , Steigler, S., & Schwabe, G. (2020). Studying with the Help of D igital \nTutors: Design Aspects of Conversational Agents that Influence the Learning Process. \nProceedings of the 53rd Hawaii International Conference on System Sciences , 3, 146 \u2013155. \nhttps://doi .org/10.24251/hicss.2020.019  \nWolfswinkel, J. F., Furtmueller, E.,  & Wilderom, C. P. M. (2013). Using grounded theory as a method \nfor rigorously reviewing literature. European Journal of Information Systems , 22(1), 45 \u201355. \nhttps://doi.org/10.1057/ejis.2011.5 1 \nWong, P. N. Y., Brumby, D. P., Babu, H. V. R., & Kobayashi, K. (2019). \u201cWatch out! \u201d Semi -\nautonomous vehicles using assertive voices to grab distracted drivers \u2019 attention. Conference on \nHuman Factors in Computing Systems - Proceedings , 5 \u201310. \nhttps://doi.o rg/10.1145/3290607.3312838  \nWorld Health Organization. (2019). World report on vision Executive Summary. World Health \nOrganization , 214(14), 1 \u201312. https://www.who.int/news -room/detail/08 -10-2019 -who-launches -\nfirst-world -report -on-vision  \nZepf, S., Gupta, A.,  Kr\u00e4mer, J. P., & Minker, W. (2020).", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}, "723cfa19-9362-42a9-b04f-f1688916185d": {"__data__": {"id_": "723cfa19-9362-42a9-b04f-f1688916185d", "embedding": null, "metadata": {"page_label": "19", "file_name": "paper.pdf", "file_path": "eigene/paper.pdf", "creation_date": "2023-11-09", "last_modified_date": "2023-10-20", "last_accessed_date": "2023-11-10"}, "excluded_embed_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "excluded_llm_metadata_keys": ["creation_date", "last_modified_date", "last_accessed_date"], "relationships": {"1": {"node_id": "517c3417-3654-4b8a-bcbd-e5f226c1cff1", "node_type": "4", "metadata": {"page_label": "19", "file_name": "paper.pdf", "file_path": "eigene/paper.pdf", "creation_date": "2023-11-09", "last_modified_date": "2023-10-20", "last_accessed_date": "2023-11-10"}, "hash": "0414edfb8bcfcaafc2a5206b8488cc0f07ec694fa622f57d14532ff03f303a77"}, "2": {"node_id": "3327329d-4bdc-4787-901d-047cb1eb6bbe", "node_type": "1", "metadata": {"page_label": "19", "file_name": "paper.pdf", "file_path": "eigene/paper.pdf", "creation_date": "2023-11-09", "last_modified_date": "2023-10-20", "last_accessed_date": "2023-11-10"}, "hash": "3e25c0b2fabd045f2555e671d2ba06f630f81dcdc96cbfe82795bab763eec7f2"}}, "hash": "9d9269722404db85cfa5b64d83213e28e5a1b8a59051a3442ccf0f4642ab7522", "text": "(2020). EmpathicSDS: Investigating Lexical and \nAcoustic Mimicry to Improve Perceived Empathy in Speech Dialogue Systems. ACM \nInternational Conference Proceeding Series . https://doi.org/10.1145/3405755.3406125  \n \nElectronic copy available at: https://ssrn.com/abstract=3910609", "start_char_idx": null, "end_char_idx": null, "text_template": "{metadata_str}\n\n{content}", "metadata_template": "{key}: {value}", "metadata_seperator": "\n"}, "__type__": "1"}}, "docstore/ref_doc_info": {"c95ed6b3-156f-4e59-8a0d-01c8133828ce": {"node_ids": ["d962463e-097f-4370-83d8-e20b4f1092b5"], "metadata": {"page_label": "1", "file_name": "paper.pdf", "file_path": "eigene/paper.pdf", "creation_date": "2023-11-09", "last_modified_date": "2023-10-20", "last_accessed_date": "2023-11-10"}}, "06548b42-8cef-403a-ad7d-f5bc17bac1ea": {"node_ids": ["47b0b74f-b680-46ea-bd0f-915a2be19cec", "96e962f9-5938-4f73-9b8b-2455ca8aed43"], "metadata": {"page_label": "2", "file_name": "paper.pdf", "file_path": "eigene/paper.pdf", "creation_date": "2023-11-09", "last_modified_date": "2023-10-20", "last_accessed_date": "2023-11-10"}}, "477b3e7f-34d7-42c6-94dd-20939b287d23": {"node_ids": ["97aeea4d-d204-40c4-8acf-3ee0db955769", "f970a434-27b3-4847-a6e9-cc2a55745a11"], "metadata": {"page_label": "3", "file_name": "paper.pdf", "file_path": "eigene/paper.pdf", "creation_date": "2023-11-09", "last_modified_date": "2023-10-20", "last_accessed_date": "2023-11-10"}}, "0aa1876e-2be2-438e-884f-77f133c1b9be": {"node_ids": ["e9b7a385-56a6-47a9-b30a-60712e34e4b1", "bfa1faf9-e0d7-4773-ad67-24844e8b97e3"], "metadata": {"page_label": "4", "file_name": "paper.pdf", "file_path": "eigene/paper.pdf", "creation_date": "2023-11-09", "last_modified_date": "2023-10-20", "last_accessed_date": "2023-11-10"}}, "fe3f9758-e169-41e7-a420-d2f454a7b3b1": {"node_ids": ["006d47bc-6903-4d82-92ca-f5699b7f2a8b", "726b4a7f-3aad-4b77-ba49-4663303a08fb"], "metadata": {"page_label": "5", "file_name": "paper.pdf", "file_path": "eigene/paper.pdf", "creation_date": "2023-11-09", "last_modified_date": "2023-10-20", "last_accessed_date": "2023-11-10"}}, "e2152268-5410-4733-b6a1-cfa6c355e475": {"node_ids": ["51431c2b-5e97-4d60-8ca2-d67984a06cb5", "e98237d3-8b3d-402b-965e-de059acd5ff5"], "metadata": {"page_label": "6", "file_name": "paper.pdf", "file_path": "eigene/paper.pdf", "creation_date": "2023-11-09", "last_modified_date": "2023-10-20", "last_accessed_date": "2023-11-10"}}, "17656815-0671-4f39-8e55-84ef48e7fce8": {"node_ids": ["a8bc167f-b8e3-4e00-9a95-de487a6e88aa", "b170b30b-8df7-4d10-bd7d-2a0930d088d7"], "metadata": {"page_label": "7", "file_name": "paper.pdf", "file_path": "eigene/paper.pdf", "creation_date": "2023-11-09", "last_modified_date": "2023-10-20", "last_accessed_date": "2023-11-10"}}, "32288d9e-325e-4485-9220-ed86c5897834": {"node_ids": ["13db9588-5abd-4d5a-8f95-69846d49d770", "d5dd142a-4ecc-43ea-bb30-960fb0e15d2d"], "metadata": {"page_label": "8", "file_name": "paper.pdf", "file_path": "eigene/paper.pdf", "creation_date": "2023-11-09", "last_modified_date": "2023-10-20", "last_accessed_date": "2023-11-10"}}, "4a4e89b4-63f8-46a9-8041-e539607181b3": {"node_ids": ["bee1e4ce-2c73-4d96-ba76-be60444c7aa9", "64342766-ba3c-4302-9c57-b8ac3418af80"], "metadata": {"page_label": "9", "file_name": "paper.pdf", "file_path": "eigene/paper.pdf", "creation_date": "2023-11-09", "last_modified_date": "2023-10-20", "last_accessed_date": "2023-11-10"}}, "966bf940-e06a-4294-96b8-1c6f319525b0": {"node_ids": ["b6909f54-10d8-47bc-b176-324f77502f86", "7ad45a9b-6e1c-4f39-96a5-3bc4b6e5e047"], "metadata": {"page_label": "10", "file_name": "paper.pdf", "file_path": "eigene/paper.pdf", "creation_date": "2023-11-09", "last_modified_date": "2023-10-20", "last_accessed_date": "2023-11-10"}}, "5343348a-9323-40cf-add1-f852bfe54ddc": {"node_ids": ["3873ba07-bdea-4dd9-b60f-91e17a28e327", "1ebd5849-4bfb-4479-899b-9be3b03c7f9f"], "metadata": {"page_label": "11", "file_name": "paper.pdf", "file_path": "eigene/paper.pdf", "creation_date": "2023-11-09", "last_modified_date": "2023-10-20", "last_accessed_date": "2023-11-10"}}, "73b1df26-3a2f-40bc-876a-a267b3b5ded5": {"node_ids": ["b9e919ab-5906-4fda-8a12-06e1feb115ff", "a1bfaad4-78d3-4411-bc23-8a947fa7bceb"], "metadata": {"page_label": "12", "file_name": "paper.pdf", "file_path": "eigene/paper.pdf", "creation_date": "2023-11-09", "last_modified_date": "2023-10-20", "last_accessed_date": "2023-11-10"}}, "fc365208-fe12-4f62-8f10-ed1b66d04c1b": {"node_ids": ["84edadf0-4632-4891-b206-b2bc13b350d6", "6261588e-302b-4a95-aa43-0ded02369dea"], "metadata": {"page_label": "13", "file_name": "paper.pdf", "file_path": "eigene/paper.pdf", "creation_date": "2023-11-09", "last_modified_date": "2023-10-20", "last_accessed_date": "2023-11-10"}}, "6f5d3c79-be03-473b-98bb-d11af3a1e2ef": {"node_ids": ["8b1417e5-1925-457b-b055-10dd94cb9927", "b7744b70-a29c-4997-a953-4cdbd7f0cee1"], "metadata": {"page_label": "14", "file_name": "paper.pdf", "file_path": "eigene/paper.pdf", "creation_date": "2023-11-09", "last_modified_date": "2023-10-20", "last_accessed_date": "2023-11-10"}}, "61670ee1-a3d2-4abb-967a-11acf5e7462e": {"node_ids": ["8eb97403-a869-4bdf-a5d8-31f7260f557a", "d859464f-8e96-4d9d-8e25-d5d76e4c4ec9"], "metadata": {"page_label": "15", "file_name": "paper.pdf", "file_path": "eigene/paper.pdf", "creation_date": "2023-11-09", "last_modified_date": "2023-10-20", "last_accessed_date": "2023-11-10"}}, "e7b68b7d-f8b6-4cbb-884b-ac86a27a6728": {"node_ids": ["b0531301-e088-4b8b-a2da-22c0b1397343", "ac6c9336-7018-445c-9555-cae5cf448e22"], "metadata": {"page_label": "16", "file_name": "paper.pdf", "file_path": "eigene/paper.pdf", "creation_date": "2023-11-09", "last_modified_date": "2023-10-20", "last_accessed_date": "2023-11-10"}}, "2d51d47a-d405-43bc-a8ca-9de1732f8548": {"node_ids": ["bd4c514c-6885-4f11-9378-ef1c4d37927f", "ca794b48-97b4-438b-b9cc-2af186639bcc"], "metadata": {"page_label": "17", "file_name": "paper.pdf", "file_path": "eigene/paper.pdf", "creation_date": "2023-11-09", "last_modified_date": "2023-10-20", "last_accessed_date": "2023-11-10"}}, "8ecc7d75-5871-4171-a74c-64872af2d05e": {"node_ids": ["3264478c-66c0-4c4a-b218-35512f5a2179", "85948b7c-d794-41f6-a9d7-bf083d155548"], "metadata": {"page_label": "18", "file_name": "paper.pdf", "file_path": "eigene/paper.pdf", "creation_date": "2023-11-09", "last_modified_date": "2023-10-20", "last_accessed_date": "2023-11-10"}}, "517c3417-3654-4b8a-bcbd-e5f226c1cff1": {"node_ids": ["3327329d-4bdc-4787-901d-047cb1eb6bbe", "723cfa19-9362-42a9-b04f-f1688916185d"], "metadata": {"page_label": "19", "file_name": "paper.pdf", "file_path": "eigene/paper.pdf", "creation_date": "2023-11-09", "last_modified_date": "2023-10-20", "last_accessed_date": "2023-11-10"}}}}